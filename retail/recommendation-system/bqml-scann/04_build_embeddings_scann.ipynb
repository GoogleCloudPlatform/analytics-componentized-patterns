{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"tf2-2-3-gpu.2-3.m59","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"name":"04_build_embeddings_scann.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ai1o-fGapfNV"},"source":["# Part 4: Create an approximate nearest neighbor index for the item embeddings\n","\n","This notebook is the fourth of five notebooks that guide you through running the [Real-time Item-to-item Recommendation with BigQuery ML Matrix Factorization and ScaNN](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/recommendation-system/bqml-scann) solution.\n","\n","Use this notebook to create an approximate nearest neighbor (ANN) index for the item embeddings by using the [ScaNN](https://github.com/google-research/google-research/tree/master/scann) framework. You create the index as a model, train the model on AI Platform Training, then export the index to Cloud Storage so that it can serve ANN information.\n","\n","Before starting this notebook, you must run the [03_create_embedding_lookup_model](03_create_embedding_lookup_model.ipynb) notebook to process the item embeddings data and export it to Cloud Storage.\n","\n","After completing this notebook, run the [05_deploy_lookup_and_scann_caip](05_deploy_lookup_and_scann_caip.ipynb) notebook to deploy the solution. Once deployed, you can submit song IDs to the solution and get similar song recommendations in return, based on the ANN index.\n"]},{"cell_type":"markdown","metadata":{"id":"Pk9Wij8ppfNY"},"source":["## Setup\r\n","\r\n","Import the required libraries, configure the environment variables, and authenticate your GCP account."]},{"cell_type":"code","metadata":{"id":"M-H_wPdmpfNY"},"source":["!pip install -q scann"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzoXTCD5pfNZ"},"source":["### Import libraries"]},{"cell_type":"code","metadata":{"id":"_7vHSotqpfNa"},"source":["import tensorflow as tf\n","import numpy as np\n","from datetime import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_tk1WOppfNa"},"source":["### Configure GCP environment settings\r\n","\r\n","Update the following variables to reflect the values for your GCP environment:\r\n","\r\n","+ `PROJECT_ID`: The ID of the Google Cloud project you are using to implement this solution.\r\n","+ `BUCKET`: The name of the Cloud Storage bucket you created to use with this solution. The `BUCKET` value should be just the bucket name, so `myBucket` rather than `gs://myBucket`.\r\n","+ `REGION`: The region to use for the AI Platform Training job."]},{"cell_type":"code","metadata":{"id":"FxhEWsL4pfNb"},"source":["PROJECT_ID = 'yourProject' # Change to your project.\n","BUCKET = 'yourBucketName' # Change to the bucket you created.\n","REGION = 'yourTrainingRegion' # Change to your AI Platform Training region.\n","EMBEDDING_FILES_PREFIX = f'gs://{BUCKET}/bqml/item_embeddings/embeddings-*'\n","OUTPUT_INDEX_DIR = f'gs://{BUCKET}/bqml/scann_index'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wPdz87S3pfNb"},"source":["### Authenticate your GCP account\n","This is required if you run the notebook in Colab. If you use an AI Platform notebook, you should already be authenticated."]},{"cell_type":"code","metadata":{"id":"fBXROib7pfNc"},"source":["try:\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  print(\"Colab user is authenticated.\")\n","except: pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaiIfZw9pfNc"},"source":["## Build the ANN index\r\n","\r\n","Use the `build` method implemented in the [indexer.py](index_builder/builder/indexer.py) module to load the embeddings from the CSV files, create the ANN index model and train it on the embedding data, and save the SavedModel file to Cloud Storage. You pass the following three parameters to this method:\r\n","\r\n","+ `embedding_files_path`, which specifies the Cloud Storage location from which to load the embedding vectors.\r\n","+ `num_leaves`, which provides the value for a hyperparameter that tunes the model based on the trade-off between retrieval latency and recall. A higher `num_leaves` value will use more data and provide better recall, but will also increase latency. If `num_leaves` is set to `None` or `0`, the `num_leaves` value is the square root of the number of items.\r\n","+ `output_dir`, which specifies the Cloud Storage location to write the ANN index SavedModel file to.\r\n","\r\n","Other configuration options for the model are set based on the [rules-of-thumb](https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md#rules-of-thumb) provided by ScaNN."]},{"cell_type":"markdown","metadata":{"id":"PwcdyrDiGcep"},"source":["### Build the index locally"]},{"cell_type":"code","metadata":{"id":"l5TGzINqpfNc"},"source":["from index_builder.builder import indexer\n","indexer.build(EMBEDDING_FILES_PREFIX, OUTPUT_INDEX_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aonq2MptpfNd"},"source":["### Build the index using AI Platform Training\r\n","\r\n","Submit an AI Platform Training job to build the ScaNN index at scale. The [index_builder](index_builder) directory contains the expected [training application packaging structure](https://cloud.google.com/ai-platform/training/docs/packaging-trainer) for submitting the AI Platform Training job."]},{"cell_type":"code","metadata":{"id":"uCVEI3mjpfNd"},"source":["if tf.io.gfile.exists(OUTPUT_INDEX_DIR):\n","  print(\"Removing {} contents...\".format(OUTPUT_INDEX_DIR))\n","  tf.io.gfile.rmtree(OUTPUT_INDEX_DIR)\n","\n","print(\"Creating output: {}\".format(OUTPUT_INDEX_DIR))\n","tf.io.gfile.makedirs(OUTPUT_INDEX_DIR)\n","\n","timestamp = datetime.utcnow().strftime('%y%m%d%H%M%S')\n","job_name = f'ks_bqml_build_scann_index_{timestamp}'\n","\n","!gcloud ai-platform jobs submit training {job_name} \\\n","  --project={PROJECT_ID} \\\n","  --region={REGION} \\\n","  --job-dir={OUTPUT_INDEX_DIR}/jobs/ \\\n","  --package-path=index_builder/builder \\\n","  --module-name=builder.task \\\n","  --config='index_builder/config.yaml' \\\n","  --runtime-version=2.2 \\\n","  --python-version=3.7 \\\n","  --\\\n","  --embedding-files-path={EMBEDDING_FILES_PREFIX} \\\n","  --output-dir={OUTPUT_INDEX_DIR} \\\n","  --num-leaves=500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MzS533hgpfNe"},"source":["After the AI Platform Training job finishes, check that the `scann_index` folder has been created in your Cloud Storage bucket:"]},{"cell_type":"code","metadata":{"id":"hgEdM632pfNe"},"source":["!gsutil ls {OUTPUT_INDEX_DIR}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcSPTZeDpfNf"},"source":["## Test the ANN index\r\n","\r\n","Test the ANN index by using the `ScaNNMatcher` class implemented in the [index_server/matching.py](index_server/matching.py) module.\r\n","\r\n","Run the following code snippets to create an item embedding from random generated values and pass it to `scann_matcher`, which returns the items IDs for the five items that are the approximate nearest neighbors of the embedding you submitted."]},{"cell_type":"code","metadata":{"id":"nQXdRKV4pfNf"},"source":["from index_server.matching import ScaNNMatcher\n","scann_matcher = ScaNNMatcher(OUTPUT_INDEX_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlUc_GsBpfNf"},"source":["vector = np.random.rand(50)\n","scann_matcher.match(vector, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CHXed9gdpfNg"},"source":["## License\n","\n","Copyright 2020 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n","\n","See the License for the specific language governing permissions and limitations under the License.\n","\n","**This is not an official Google product but sample code provided for an educational purpose**"]}]}
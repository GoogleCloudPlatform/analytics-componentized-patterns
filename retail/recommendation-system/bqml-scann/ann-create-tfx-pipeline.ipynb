{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-latency item-to-item recommendation system \n",
    "\n",
    "## Part 3 - Orchestrating with TFX\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is a part of the [**Low-latency item-to-item recommendation system** ML Engineering blueprint](https://github.com/jarokaz/analytics-componentized-patterns/tree/master/retail/recommendation-system/bqml-ann).\n",
    "\n",
    "The blueprint provides guidance and code samples for how to develop and operationalize a near real-time item-to-itme recommendations system that utilizes BigQuery, BigQuery ML and AI Platform ANN Service.\n",
    "\n",
    "This notebook demonstrates how to use TFX and AI Platform Pipelines (Unified) to operationalize the workflow of creating embeddings and building and deploying an ANN Service index. \n",
    "\n",
    "In the notebook you go through the following steps.\n",
    "\n",
    "1. Creating TFX custom components that encapsulate operations on BQ, BQML and ANN Service.\n",
    "2. Creating a TFX pipeline that automates the processes you went through manually in Part 1 and Part 3. \n",
    "3. Testing the pipeline locally using Beam runner.\n",
    "4. Compiling the pipeline to the TFX IR format for execution to AI Platform Pipelines (Unified).\n",
    "5. Submitting pipeline runs.\n",
    "\n",
    "This notebook was designed to run on [AI Platform Notebooks](https://cloud.google.com/ai-platform/notebooks/docs) using the standard TensorFlow 2.3 image. Your notebook instance should be in the same project as the AI Platform ANN Service.\n",
    "\n",
    "While AI Platform ANN Service is in the Experimental stage your project must be allow-listed before using the service. Contact `gcp-ann-feedback@` to allow list your project and user id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX Version:  0.25.0\n",
      "KFP Version:  1.1.2\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import kfp\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "from aiplatform.pipelines import client\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "\n",
    "print('TFX Version: ', tfx.__version__)\n",
    "print('KFP Version: ', kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the notebook's environment\n",
    "\n",
    "### Configure GCP environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n",
    "```sh\n",
    "gcloud auth login\n",
    "```\n",
    "**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following constants to the values reflecting your environment:\n",
    "\n",
    "* `PROJECT_ID` - your GCP project ID\n",
    "* `PROJECT_NUMBER` - your GCP project number\n",
    "* `BUCKET_NAME` - a name of the GCS bucket that will be used to host artifacts created by the pipeline\n",
    "* `PIPELINE_NAME_SUFFIX` - a suffix appended to the standard pipeline name. You can change to differentiate between pipelines from different users in a classroom environment\n",
    "* `API_KEY` - a GCP API key\n",
    "* `VPC_NAME` - a name of the GCP VPC to use for the index deployments.  \n",
    "* `REGION` - a compute region. Don't change the default - `us-central` - while the ANN Service is in the experimental stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n",
      "PIPELINE_ROOT: gs://jk-ann-staging/pipeline_root/ann-pipeline-jarekk\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'jk-mlops-dev'  # <---CHANGE THIS\n",
    "PROJECT_NUMBER = '895222332033'  # <---CHANGE THIS\n",
    "API_KEY = ''  # <---CHANGE THIS\n",
    "USER = 'jarekk' # <---CHANGE THIS\n",
    "BUCKET_NAME = 'jk-ann-staging'  # <---CHANGE THIS\n",
    "VPC_NAME = 'default' # <---CHANGE THIS IF USING A DIFFERENT VPC\n",
    "\n",
    "REGION = 'us-central1'\n",
    "PIPELINE_NAME = \"ann-pipeline-{}\".format(USER)\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, PIPELINE_NAME)\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "    \n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run your pipeline using the default AI Platform Pipelines service account. The account needs `roles/aiplatform.user` permissions to access the ANN Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for project [jk-mlops-dev].\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.admin\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.customCodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:aip-training@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:caip-training@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-895222332033@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.user\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-automl.iam.gserviceaccount.com\n",
      "  role: roles/automl.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-895222332033@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.admin\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.user\n",
      "- members:\n",
      "  - serviceAccount:895222332033@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.editor\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/cloudsql.admin\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-vm@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/cloudtrace.agent\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/compute.networkAdmin\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.admin\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.developer\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/dataproc.editor\n",
      "- members:\n",
      "  - serviceAccount:895222332033-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:895222332033@cloudservices.gserviceaccount.com\n",
      "  - serviceAccount:jk-mlops-dev@appspot.gserviceaccount.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@firebase-rules.iam.gserviceaccount.com\n",
      "  role: roles/firebaserules.system\n",
      "- members:\n",
      "  - user:jarekk@gcp.solutions\n",
      "  role: roles/iap.httpsResourceAccessor\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-vm@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/logging.logWriter\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-vm@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/meshtelemetry.reporter\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/ml.admin\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-vm@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/monitoring.metricWriter\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-vm@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/monitoring.viewer\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:mgmt-cluster-cnrm-system@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - user:jarekk@gcp.solutions\n",
      "  - user:jarekk@google.com\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/servicemanagement.admin\n",
      "- members:\n",
      "  - serviceAccount:service-895222332033@service-networking.iam.gserviceaccount.com\n",
      "  role: roles/servicenetworking.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/source.admin\n",
      "- members:\n",
      "  - serviceAccount:aip-training@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:caip-training@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/storage.admin\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-vm@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/storage.objectViewer\n",
      "- members:\n",
      "  - serviceAccount:kubeflowdemo-admin@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  - serviceAccount:kubeflowdemo-user@jk-mlops-dev.iam.gserviceaccount.com\n",
      "  role: roles/viewer\n",
      "etag: BwW3JXFmmfY=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
    "--member=serviceAccount:service-{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com \\\n",
    "--role=roles/aiplatform.admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining custom components\n",
    "\n",
    "In this section of the notebook you define a set of custom TFX components that encapsulate BQ, BQML and ANN Service calls. The components are [TFX Custom Python function components](https://www.tensorflow.org/tfx/guide/custom_function_component). \n",
    "\n",
    "Each component is created as a separate Python module. You also create a couple of helper modules that encapsulate Python functions and classess used across the custom components. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove files created in the previous executions of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating component folder\n"
     ]
    }
   ],
   "source": [
    "component_folder = 'bq_components'\n",
    "\n",
    "if tf.io.gfile.exists(component_folder):\n",
    "    print('Removing older file')\n",
    "    tf.io.gfile.rmtree(component_folder)\n",
    "print('Creating component folder')\n",
    "tf.io.gfile.mkdir(component_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/analytics-componentized-patterns/retail/recommendation-system/bqml-ann/bq_components/bq_components\n"
     ]
    }
   ],
   "source": [
    "%cd {component_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom types for ANN service artifacts\n",
    "\n",
    "This module defines a couple of custom TFX artifacts to track ANN Service indexes and index deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ann_types.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ann_types.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Custom types for managing ANN artifacts.\"\"\"\n",
    "\n",
    "from tfx.types import artifact\n",
    "\n",
    "class ANNIndex(artifact.Artifact):\n",
    "    TYPE_NAME = 'ANNIndex'\n",
    "    \n",
    "class DeployedANNIndex(artifact.Artifact):\n",
    "    TYPE_NAME = 'DeployedANNIndex'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a wrapper around ANN Service REST API\n",
    "\n",
    "This module provides a convenience wrapper around ANN Service REST API. In the experimental stage, the ANN Service does not have an \"official\" Python client SDK nor it is supported by the Google Discovery API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ann_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ann_service.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Helper classes encapsulating ANN Service REST API.\"\"\"\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "\n",
    "import google.auth\n",
    "\n",
    "class ANNClient(object):\n",
    "    \"\"\"Base ANN Service client.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id, project_number, region):\n",
    "        credentials, _ = google.auth.default()\n",
    "        self.authed_session = google.auth.transport.requests.AuthorizedSession(credentials)\n",
    "        self.ann_endpoint = f'{region}-aiplatform.googleapis.com'\n",
    "        self.ann_parent = f'https://{self.ann_endpoint}/v1alpha1/projects/{project_id}/locations/{region}'\n",
    "        self.project_id = project_id\n",
    "        self.project_number = project_number\n",
    "        self.region = region\n",
    "        \n",
    "    def wait_for_completion(self, operation_id, message, sleep_time):\n",
    "        \"\"\"Waits for a completion of a long running operation.\"\"\"\n",
    "        \n",
    "        api_url = f'{self.ann_parent}/operations/{operation_id}'\n",
    "\n",
    "        start_time = datetime.datetime.utcnow()\n",
    "        while True:\n",
    "            response = self.authed_session.get(api_url)\n",
    "            if response.status_code != 200:\n",
    "                raise RuntimeError(response.json())\n",
    "            if 'done' in response.json().keys():\n",
    "                logging.info('Operation completed!')\n",
    "                break\n",
    "            elapsed_time = datetime.datetime.utcnow() - start_time\n",
    "            logging.info('{}. Elapsed time since start: {}.'.format(\n",
    "                message, str(elapsed_time)))\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "        return response.json()['response']\n",
    "\n",
    "\n",
    "class IndexClient(ANNClient):\n",
    "    \"\"\"Encapsulates a subset of control plane APIs \n",
    "    that manage ANN indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, project_id, project_number, region):\n",
    "        super().__init__(project_id, project_number, region)\n",
    "\n",
    "    def create_index(self, display_name, description, metadata):\n",
    "        \"\"\"Creates an ANN Index.\"\"\"\n",
    "    \n",
    "        api_url = f'{self.ann_parent}/indexes'\n",
    "    \n",
    "        request_body = {\n",
    "            'display_name': display_name,\n",
    "            'description': description,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    \n",
    "        response = self.authed_session.post(api_url, data=json.dumps(request_body))\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(response.text)\n",
    "        operation_id = response.json()['name'].split('/')[-1]\n",
    "        \n",
    "        return operation_id\n",
    "\n",
    "    def list_indexes(self, display_name=None):\n",
    "        \"\"\"Lists all indexes with a given display name or\n",
    "        all indexes if the display_name is not provided.\"\"\"\n",
    "    \n",
    "        if display_name:\n",
    "            api_url = f'{self.ann_parent}/indexes?filter=display_name=\"{display_name}\"'\n",
    "        else:\n",
    "            api_url = f'{self.ann_parent}/indexes'\n",
    "\n",
    "        response = self.authed_session.get(api_url).json()\n",
    "\n",
    "        return response['indexes'] if response else []\n",
    "    \n",
    "    def delete_index(self, index_id):\n",
    "        \"\"\"Deletes an ANN index.\"\"\"\n",
    "        \n",
    "        api_url = f'{self.ann_parent}/indexes/{index_id}'\n",
    "        response = self.authed_session.delete(api_url)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(response.text)\n",
    "\n",
    "\n",
    "class IndexDeploymentClient(ANNClient):\n",
    "    \"\"\"Encapsulates a subset of control plane APIs \n",
    "    that manage ANN endpoints and deployments.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id, project_number, region):\n",
    "        super().__init__(project_id, project_number, region)\n",
    "\n",
    "    def create_endpoint(self, display_name, vpc_name):\n",
    "        \"\"\"Creates an ANN endpoint.\"\"\"\n",
    "    \n",
    "        api_url = f'{self.ann_parent}/indexEndpoints'\n",
    "        network_name = f'projects/{self.project_number}/global/networks/{vpc_name}'\n",
    "\n",
    "        request_body = {\n",
    "            'display_name': display_name,\n",
    "            'network': network_name\n",
    "        }\n",
    "\n",
    "        response = self.authed_session.post(api_url, data=json.dumps(request_body))\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(response.text)\n",
    "        operation_id = response.json()['name'].split('/')[-1]\n",
    "    \n",
    "        return operation_id\n",
    "    \n",
    "    def list_endpoints(self, display_name=None):\n",
    "        \"\"\"Lists all ANN endpoints with a given display name or\n",
    "        all endpoints in the project if the display_name is not provided.\"\"\"\n",
    "        \n",
    "        if display_name:\n",
    "            api_url = f'{self.ann_parent}/indexEndpoints?filter=display_name=\"{display_name}\"'\n",
    "        else:\n",
    "            api_url = f'{self.ann_parent}/indexEndpoints'\n",
    "\n",
    "        response = self.authed_session.get(api_url).json()\n",
    " \n",
    "        return response['indexEndpoints'] if response else []\n",
    "    \n",
    "    def delete_endpoint(self, endpoint_id):\n",
    "        \"\"\"Deletes an ANN endpoint.\"\"\"\n",
    "        \n",
    "        api_url = f'{self.ann_parent}/indexEndpoints/{endpoint_id}'\n",
    "        \n",
    "        response = self.authed_session.delete(api_url)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(response.text)\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    def create_deployment(self, display_name, deployment_id, endpoint_id, index_id):\n",
    "        \"\"\"Deploys an ANN index to an endpoint.\"\"\"\n",
    "    \n",
    "        api_url = f'{self.ann_parent}/indexEndpoints/{endpoint_id}:deployIndex'\n",
    "        index_name = f'projects/{self.project_number}/locations/{self.region}/indexes/{index_id}'\n",
    "\n",
    "        request_body = {\n",
    "            'deployed_index': {\n",
    "                'id': deployment_id,\n",
    "                'index': index_name,\n",
    "                'display_name': display_name\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = self.authed_session.post(api_url, data=json.dumps(request_body))\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(response.text)\n",
    "        operation_id = response.json()['name'].split('/')[-1]\n",
    "        \n",
    "        return operation_id\n",
    "    \n",
    "    def get_deployment_grpc_ip(self, endpoint_id, deployment_id):\n",
    "        \"\"\"Returns a private IP address for a gRPC interface to \n",
    "        an Index deployment.\"\"\"\n",
    "  \n",
    "        api_url = f'{self.ann_parent}/indexEndpoints/{endpoint_id}'\n",
    "\n",
    "        response = self.authed_session.get(api_url)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(response.text)\n",
    "            \n",
    "        endpoint_ip = None\n",
    "        if 'deployedIndexes' in response.json().keys():\n",
    "            for deployment in response.json()['deployedIndexes']:\n",
    "                if deployment['id'] == deployment_id:\n",
    "                    endpoint_ip = deployment['privateEndpoints']['matchGrpcAddress']\n",
    "                    \n",
    "        return endpoint_ip\n",
    "\n",
    "    \n",
    "    def delete_deployment(self, endpoint_id, deployment_id):\n",
    "        \"\"\"Undeployes an index from an endpoint.\"\"\"\n",
    "        \n",
    "        api_url = f'{self.ann_parent}/indexEndpoints/{endpoint_id}:undeployIndex'\n",
    "        \n",
    "        request_body = {\n",
    "            'deployed_index_id': deployment_id\n",
    "        }\n",
    "    \n",
    "        response = self.authed_session.post(api_url, data=json.dumps(request_body))\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(response.text)\n",
    "        \n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute PMI component\n",
    "\n",
    "This component encapsulates a call to the BigQuery stored procedure that calculates item cooccurence. Refer to Part 1 for more details about item coocurrent calculations.\n",
    "\n",
    "The component tracks the output `item_cooc` table created by the stored procedure using the TFX (simple) Dataset artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing compute_pmi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile compute_pmi.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"BigQuery compute PMI component.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
    "\n",
    "from tfx.types.experimental.simple_artifacts import Dataset as BQDataset\n",
    "\n",
    "\n",
    "@component\n",
    "def compute_pmi(\n",
    "    project_id: Parameter[str],\n",
    "    bq_dataset: Parameter[str],\n",
    "    min_item_frequency: Parameter[int],\n",
    "    max_group_size: Parameter[int],\n",
    "    item_cooc: OutputArtifact[BQDataset]):\n",
    "    \n",
    "    stored_proc = f'{bq_dataset}.sp_ComputePMI'\n",
    "    query = f'''\n",
    "        DECLARE min_item_frequency INT64;\n",
    "        DECLARE max_group_size INT64;\n",
    "\n",
    "        SET min_item_frequency = {min_item_frequency};\n",
    "        SET max_group_size = {max_group_size};\n",
    "\n",
    "        CALL {stored_proc}(min_item_frequency, max_group_size);\n",
    "    '''\n",
    "    result_table = 'item_cooc'\n",
    "\n",
    "    logging.info(f'Starting computing PMI...')\n",
    "  \n",
    "    #client = bigquery.Client(project=project_id)\n",
    "    #query_job = client.query(query)\n",
    "    #query_job.result() # Wait for the job to complete\n",
    "  \n",
    "    logging.info(f'Items PMI computation completed. Output in {bq_dataset}.{result_table}.')\n",
    "  \n",
    "    # Write the location of the output table to metadata.  \n",
    "    item_cooc.set_string_custom_property('table_name',\n",
    "                                         f'{project_id}:{bq_dataset}.{result_table}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train Item Matching Model component\n",
    "\n",
    "This component encapsulates a call to the BigQuery stored procedure that trains the BQML Matrix Factorization model. Refer to Part 1 for more details about model training.\n",
    "\n",
    "The component tracks the output `item_matching_model` BQML model created by the stored procedure using the TFX (simple) Model artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_item_matching.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_item_matching.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"BigQuery compute PMI component.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
    "\n",
    "from tfx.types.experimental.simple_artifacts import Dataset as BQDataset\n",
    "from tfx.types.standard_artifacts import Model as BQModel\n",
    "\n",
    "\n",
    "@component\n",
    "def train_item_matching_model(\n",
    "    project_id: Parameter[str],\n",
    "    bq_dataset: Parameter[str],\n",
    "    dimensions: Parameter[int],\n",
    "    item_cooc: InputArtifact[BQDataset],\n",
    "    bq_model: OutputArtifact[BQModel]):\n",
    "    \n",
    "    item_cooc_table = item_cooc.get_string_custom_property('table_name')\n",
    "    stored_proc = f'{bq_dataset}.sp_TrainItemMatchingModel'\n",
    "    query = f'''\n",
    "        DECLARE dimensions INT64 DEFAULT {dimensions};\n",
    "        CALL {stored_proc}(dimensions);\n",
    "    '''\n",
    "    model_name = 'item_matching_model'\n",
    "  \n",
    "    logging.info(f'Using item co-occurrence table: item_cooc_table')\n",
    "    logging.info(f'Starting training of the model...')\n",
    "    \n",
    "    #client = bigquery.Client(project=project_id)\n",
    "    #query_job = client.query(query)\n",
    "    #query_job.result()\n",
    "  \n",
    "    logging.info(f'Model training completed. Output in {bq_dataset}.{model_name}.')\n",
    "  \n",
    "    # Write the location of the model to metadata. \n",
    "    bq_model.set_string_custom_property('model_name',\n",
    "                                         f'{project_id}:{bq_dataset}.{model_name}')\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Extract Embeddings component\n",
    "\n",
    "This component encapsulates a call to the BigQuery stored procedure that extracts embdeddings from the model to the staging table. Refer to Part 1 for more details about embeddings extraction.\n",
    "\n",
    "The component tracks the output `item_embeddings` table created by the stored procedure using the TFX (simple) Dataset artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing extract_embeddings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile extract_embeddings.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Extracts embeddings to a BQ table.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
    "\n",
    "from tfx.types.experimental.simple_artifacts import Dataset as BQDataset \n",
    "from tfx.types.standard_artifacts import Model as BQModel\n",
    "\n",
    "\n",
    "@component\n",
    "def extract_embeddings(\n",
    "    project_id: Parameter[str],\n",
    "    bq_dataset: Parameter[str],\n",
    "    bq_model: InputArtifact[BQModel],\n",
    "    item_embeddings: OutputArtifact[BQDataset]):\n",
    "  \n",
    "    embedding_model_name = bq_model.get_string_custom_property('model_name')\n",
    "    stored_proc = f'{bq_dataset}.sp_ExractEmbeddings'\n",
    "    query = f'''\n",
    "        CALL {stored_proc}();\n",
    "    '''\n",
    "    embeddings_table = 'item_embeddings'\n",
    "\n",
    "    logging.info(f'Extracting item embeddings from: {embedding_model_name}')\n",
    "    \n",
    "    #client = bigquery.Client(project=project_id)\n",
    "    #query_job = client.query(query)\n",
    "    #query_job.result() # Wait for the job to complete\n",
    "  \n",
    "    logging.info(f'Embeddings extraction completed. Output in {bq_dataset}.{embeddings_table}')\n",
    "  \n",
    "    # Write the location of the output table to metadata.\n",
    "    item_embeddings.set_string_custom_property('table_name', \n",
    "                                                f'{project_id}:{bq_dataset}.{embeddings_table}')\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Export Embeddings component\n",
    "\n",
    "This component encapsulates a BigQuery table extraction job that extracts the `item_embeddings` table to a GCS location as files in the JSONL format. The format of the extracted files is compatible with the ingestion schema for the ANN Service.\n",
    "\n",
    "The component tracks the output files the TFX (simple) Dataset artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing export_embeddings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile export_embeddings.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Exports embeddings from a BQ table to a GCS location.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
    "\n",
    "from tfx.types.experimental.simple_artifacts import Dataset \n",
    "\n",
    "BQDataset = Dataset\n",
    "\n",
    "@component\n",
    "def export_embeddings(\n",
    "    project_id: Parameter[str],\n",
    "    gcs_location: Parameter[str],\n",
    "    item_embeddings_bq: InputArtifact[BQDataset],\n",
    "    item_embeddings_gcs: OutputArtifact[Dataset]):\n",
    "    \n",
    "    filename_pattern = 'embedding-*.json'\n",
    "    gcs_location = gcs_location.rstrip('/')\n",
    "    destination_uri = f'{gcs_location}/{filename_pattern}'\n",
    "    \n",
    "    _, table_name = item_embeddings_bq.get_string_custom_property('table_name').split(':')\n",
    "  \n",
    "    logging.info(f'Exporting item embeddings from: {table_name}')\n",
    "  \n",
    "    bq_dataset, table_id = table_name.split('.')\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset_ref = bigquery.DatasetReference(project_id, bq_dataset)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config = bigquery.job.ExtractJobConfig()\n",
    "    job_config.destination_format = bigquery.DestinationFormat.NEWLINE_DELIMITED_JSON\n",
    "\n",
    "    #extract_job = client.extract_table(\n",
    "    #    table_ref,\n",
    "    #    destination_uris=destination_uri,\n",
    "    #    job_config=job_config\n",
    "    #)  \n",
    "    #extract_job.result() # Wait for resuls\n",
    "    \n",
    "    logging.info(f'Embeddings export completed. Output in {gcs_location}')\n",
    "  \n",
    "    # Write the location of the embeddings to metadata.\n",
    "    item_embeddings_gcs.uri = gcs_location\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ANN index component\n",
    "\n",
    "This component encapsulats the calls to the ANN Service to create an ANN Index. \n",
    "\n",
    "The component tracks the created index int the TFX custom `ANNIndex` artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_index.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Creates an ANN index.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import google.auth\n",
    "import numpy as np\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
    "from tfx.types.experimental.simple_artifacts import Dataset \n",
    "\n",
    "from ann_service import IndexClient\n",
    "from ann_types import ANNIndex\n",
    "\n",
    "NUM_NEIGHBOURS = 10\n",
    "MAX_LEAVES_TO_SEARCH = 200\n",
    "METRIC = 'DOT_PRODUCT_DISTANCE'\n",
    "FEATURE_NORM_TYPE = 'UNIT_L2_NORM'\n",
    "CHILD_NODE_COUNT = 1000\n",
    "APPROXIMATE_NEIGHBORS_COUNT = 50\n",
    "\n",
    "@component\n",
    "def create_index(\n",
    "    project_id: Parameter[str],\n",
    "    project_number: Parameter[str],\n",
    "    region: Parameter[str],\n",
    "    display_name: Parameter[str],\n",
    "    dimensions: Parameter[int],\n",
    "    item_embeddings: InputArtifact[Dataset],\n",
    "    ann_index: OutputArtifact[ANNIndex]):\n",
    "    \n",
    "    index_client = IndexClient(project_id, project_number, region)\n",
    "    \n",
    "    logging.info('Creating index:')\n",
    "    logging.info(f'    Index display name: {display_name}')\n",
    "    logging.info(f'    Embeddings location: {item_embeddings.uri}')\n",
    "    \n",
    "    index_description = display_name\n",
    "    index_metadata = {\n",
    "        'contents_delta_uri': item_embeddings.uri,\n",
    "        'config': {\n",
    "            'dimensions': dimensions,\n",
    "            'approximate_neighbors_count': APPROXIMATE_NEIGHBORS_COUNT,\n",
    "            'distance_measure_type': METRIC,\n",
    "            'feature_norm_type': FEATURE_NORM_TYPE,\n",
    "            'tree_ah_config': {\n",
    "                'child_node_count': CHILD_NODE_COUNT,\n",
    "                'max_leaves_to_search': MAX_LEAVES_TO_SEARCH\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #operation_id = index_client.create_index(display_name, \n",
    "    #                                         index_description,\n",
    "    #                                         index_metadata)\n",
    "    #response = index_client.wait_for_completion(operation_id, 'Waiting for ANN index', 45)\n",
    "    #index_name = response['name']\n",
    "    \n",
    "    index_name = 'projects/895222332033/locations/us-central1/indexes/1160802803954745344'\n",
    "    \n",
    "    logging.info('Index {} created.'.format(index_name))\n",
    "  \n",
    "    # Write the index name to metadata.\n",
    "    ann_index.set_string_custom_property('index_name', \n",
    "                                         index_name)\n",
    "    ann_index.set_string_custom_property('index_display_name', \n",
    "                                         display_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy ANN index component\n",
    "\n",
    "This component deploys an ANN index to an ANN Endpoint. \n",
    "\n",
    "The componet tracks the deployed index in the TFX custom `DeployedANNIndex` artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy_index.py\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Deploys an ANN index.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
    "from tfx.types.experimental.simple_artifacts import Dataset \n",
    "\n",
    "from ann_service import IndexDeploymentClient\n",
    "from ann_types import ANNIndex\n",
    "from ann_types import DeployedANNIndex\n",
    "\n",
    "\n",
    "@component\n",
    "def deploy_index(\n",
    "    project_id: Parameter[str],\n",
    "    project_number: Parameter[str],\n",
    "    region: Parameter[str],\n",
    "    vpc_name: Parameter[str],\n",
    "    deployed_index_id: Parameter[str],\n",
    "    ann_index: InputArtifact[ANNIndex],\n",
    "    deployed_ann_index: OutputArtifact[DeployedANNIndex]\n",
    "    ):\n",
    "    \n",
    "    deployment_client = IndexDeploymentClient(project_id, \n",
    "                                              project_number,\n",
    "                                              region)\n",
    "    \n",
    "    index_name = ann_index.get_string_custom_property('index_name')\n",
    "    index_display_name = ann_index.get_string_custom_property('index_display_name')\n",
    "    endpoint_display_name = f'Endpoint for {index_display_name}'\n",
    "    \n",
    "    logging.info(f'Creating endpoint: {endpoint_display_name}')\n",
    "    operation_id = deployment_client.create_endpoint(endpoint_display_name, vpc_name)\n",
    "    response = deployment_client.wait_for_completion(operation_id, 'Waiting for endpoint', 30)\n",
    "    endpoint_name = response['name']\n",
    "    logging.info(f'Endpoint created: {endpoint_name}')\n",
    "  \n",
    "    #logging.info(f'Creating deployed index: {deployed_index_id}')\n",
    "    #logging.info(f'                  from: {index_name}')\n",
    "    #endpoint_id = endpoint_name.split('/')[-1]\n",
    "    #index_id = index_name.split('/')[-1]\n",
    "    #deployed_index_display_name = f'Deployed {index_display_name}'\n",
    "    #operation_id = deployment_client.create_deployment(\n",
    "    #    deployed_index_display_name, \n",
    "    #    deployed_index_id,\n",
    "    #    endpoint_id,\n",
    "    #    index_id)\n",
    "\n",
    "    #response = deployment_client.wait_for_completion(operation_id, 'Waiting for deployment', 60)\n",
    "    #logging.info('Index deployed!')\n",
    "  \n",
    "    #deployed_index_ip = deployment_client.get_deployment_grpc_ip(\n",
    "    #    endpoint_id, deployed_index_id\n",
    "    #)\n",
    "    # Write the deployed index properties to metadata.\n",
    "    deployed_ann_index.set_string_custom_property('endpoint_name', \n",
    "                                                  endpoint_name)\n",
    "    #deployed_ann_index.set_string_custom_property('deployed_index_id', \n",
    "    #                                              deployed_index_id)\n",
    "    #deployed_ann_index.set_string_custom_property('index_name', \n",
    "    #                                              index_name)\n",
    "    #deployed_ann_index.set_string_custom_property('deployed_index_grpc_ip', \n",
    "    #                                              deployed_index_ip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TFX pipeline\n",
    "\n",
    "The pipeline automates the process of preparing item embeddings (in BigQuery), training a Matrix Factorization model (in BQML), and creating and deploying an ANN Service index.\n",
    "\n",
    "The pipeline has a simple sequential flow. The pipeline accepts a set of runtime parameters that define GCP environment settings and embeddings and index assembly parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Only required for local run.\n",
    "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
    "\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
    "\n",
    "from compute_pmi import compute_pmi\n",
    "from export_embeddings import export_embeddings\n",
    "from extract_embeddings import extract_embeddings\n",
    "from train_item_matching import train_item_matching_model\n",
    "from create_index import create_index\n",
    "from deploy_index import deploy_index\n",
    "\n",
    "def ann_pipeline(\n",
    "    pipeline_name,\n",
    "    pipeline_root,\n",
    "    metadata_connection_config,\n",
    "    project_id,\n",
    "    project_number,\n",
    "    region,\n",
    "    vpc_name,\n",
    "    bq_dataset_name,\n",
    "    min_item_frequency,\n",
    "    max_group_size,\n",
    "    dimensions,\n",
    "    embeddings_gcs_location,\n",
    "    index_display_name,\n",
    "    deployed_index_id) -> Pipeline:\n",
    "    \"\"\"Implements the SCANN training pipeline.\"\"\"\n",
    " \n",
    "    pmi_computer = compute_pmi(\n",
    "        project_id=project_id,\n",
    "        bq_dataset=bq_dataset_name,\n",
    "        min_item_frequency=min_item_frequency,\n",
    "        max_group_size=max_group_size\n",
    "    )\n",
    "    \n",
    "    bqml_trainer = train_item_matching_model(\n",
    "        project_id=project_id,\n",
    "        bq_dataset=bq_dataset_name,\n",
    "        item_cooc=pmi_computer.outputs.item_cooc,\n",
    "        dimensions=dimensions,\n",
    "    )\n",
    "    \n",
    "    embeddings_extractor = extract_embeddings(\n",
    "        project_id=project_id,\n",
    "        bq_dataset=bq_dataset_name,\n",
    "        bq_model=bqml_trainer.outputs.bq_model\n",
    "    )\n",
    "    \n",
    "    embeddings_exporter = export_embeddings(\n",
    "        project_id=project_id,\n",
    "        gcs_location=embeddings_gcs_location,\n",
    "        item_embeddings_bq=embeddings_extractor.outputs.item_embeddings\n",
    "    )\n",
    "    \n",
    "    index_constructor = create_index(\n",
    "        project_id=project_id,\n",
    "        project_number=project_number,\n",
    "        region=region,\n",
    "        display_name=index_display_name,\n",
    "        dimensions=dimensions,\n",
    "        item_embeddings=embeddings_exporter.outputs.item_embeddings_gcs\n",
    "    )\n",
    "    \n",
    "    index_deployer = deploy_index(\n",
    "        project_id=project_id,\n",
    "        project_number=project_number,\n",
    "        region=region,\n",
    "        vpc_name=vpc_name,\n",
    "        deployed_index_id=deployed_index_id,\n",
    "        ann_index=index_constructor.outputs.ann_index\n",
    "    )\n",
    "\n",
    "    components = [\n",
    "        pmi_computer,\n",
    "        bqml_trainer,\n",
    "        embeddings_extractor,\n",
    "        embeddings_exporter,\n",
    "        index_constructor,\n",
    "        index_deployer\n",
    "    ]\n",
    "    \n",
    "    return Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        # Only needed for local runs.\n",
    "        metadata_connection_config=metadata_connection_config,\n",
    "        components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the pipeline locally\n",
    "\n",
    "You will first run the pipeline locally using the Beam runner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the metadata and artifacts from the previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous artifacts...\n",
      "Removing local mlmd SQLite...\n",
      "Creating mlmd directory:  /tmp/mlmd\n",
      "Creating pipeline root folder:  /tmp/ann-pipeline-jarekk\n"
     ]
    }
   ],
   "source": [
    "pipeline_root = f'/tmp/{PIPELINE_NAME}'\n",
    "local_mlmd_folder = '/tmp/mlmd'\n",
    "\n",
    "if tf.io.gfile.exists(pipeline_root):\n",
    "  print(\"Removing previous artifacts...\")\n",
    "  tf.io.gfile.rmtree(pipeline_root)\n",
    "if tf.io.gfile.exists(local_mlmd_folder):\n",
    "  print(\"Removing local mlmd SQLite...\")\n",
    "  tf.io.gfile.rmtree(local_mlmd_folder)\n",
    "print(\"Creating mlmd directory: \", local_mlmd_folder)\n",
    "tf.io.gfile.mkdir(local_mlmd_folder)\n",
    "print(\"Creating pipeline root folder: \", pipeline_root)\n",
    "tf.io.gfile.mkdir(pipeline_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pipeline parameters and create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset_name = 'song_embeddings'\n",
    "index_display_name = 'Song embeddings'\n",
    "deployed_index_id = 'deployed_song_embeddings_91'\n",
    "min_item_frequency = 15\n",
    "max_group_size = 100\n",
    "dimensions = 50\n",
    "embeddings_gcs_location = f'gs://{BUCKET_NAME}/embeddings'\n",
    "\n",
    "metadata_connection_config = sqlite_metadata_connection_config(\n",
    "    os.path.join(local_mlmd_folder, 'metadata.sqlite'))\n",
    "\n",
    "pipeline = ann_pipeline(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=pipeline_root,\n",
    "    metadata_connection_config=metadata_connection_config,\n",
    "    project_id=PROJECT_ID,\n",
    "    project_number=PROJECT_NUMBER,\n",
    "    region=REGION,\n",
    "    vpc_name=VPC_NAME,\n",
    "    bq_dataset_name=bq_dataset_name,\n",
    "    index_display_name=index_display_name,\n",
    "    deployed_index_id=deployed_index_id,\n",
    "    min_item_frequency=min_item_frequency,\n",
    "    max_group_size=max_group_size,\n",
    "    dimensions=dimensions,\n",
    "    embeddings_gcs_location=embeddings_gcs_location\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "BeamDagRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect produced metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = os.path.join(local_mlmd_folder, 'metadata.sqlite')\n",
    "connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n",
    "store = metadata_store.MetadataStore(connection_config)\n",
    "store.get_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline on AI Platform Pipelines\n",
    "\n",
    "You will now run the pipeline on AI Platform Pipelines (Unified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package custom components into a container\n",
    "\n",
    "The modules containing custom components must be first package as a docker container image, which is a derivative of the standard TFX image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM gcr.io/tfx-oss-public/tfx:0.25.0\n",
    "WORKDIR /pipeline\n",
    "COPY ./ ./\n",
    "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and push the docker image to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 9 file(s) totalling 22.4 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://jk-mlops-dev_cloudbuild/source/1608746412.767462-5cd71c1c10824fdc933dc27e175b4ff1.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-mlops-dev/builds/1cee92eb-8cd8-4720-a72f-aeaf10ddca16].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/1cee92eb-8cd8-4720-a72f-aeaf10ddca16?project=895222332033].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"1cee92eb-8cd8-4720-a72f-aeaf10ddca16\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-mlops-dev_cloudbuild/source/1608746412.767462-5cd71c1c10824fdc933dc27e175b4ff1.tgz#1608746413099293\n",
      "Copying gs://jk-mlops-dev_cloudbuild/source/1608746412.767462-5cd71c1c10824fdc933dc27e175b4ff1.tgz#1608746413099293...\n",
      "/ [1 files][  4.5 KiB/  4.5 KiB]                                                \n",
      "Operation completed over 1 objects/4.5 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  30.21kB\n",
      "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:0.25.0\n",
      "0.25.0: Pulling from tfx-oss-public/tfx\n",
      "bd47987755ba: Pulling fs layer\n",
      "831c222b21d8: Pulling fs layer\n",
      "3c2cba919283: Pulling fs layer\n",
      "e378d88a5f59: Pulling fs layer\n",
      "df37508d2f5c: Pulling fs layer\n",
      "c28e7cc900d1: Pulling fs layer\n",
      "9019978541a7: Pulling fs layer\n",
      "80dc388c898c: Pulling fs layer\n",
      "afebcf787e04: Pulling fs layer\n",
      "b32cc9704312: Pulling fs layer\n",
      "a0336ba74309: Pulling fs layer\n",
      "e378d88a5f59: Waiting\n",
      "df37508d2f5c: Waiting\n",
      "c28e7cc900d1: Waiting\n",
      "9019978541a7: Waiting\n",
      "80dc388c898c: Waiting\n",
      "afebcf787e04: Waiting\n",
      "b32cc9704312: Waiting\n",
      "a0336ba74309: Waiting\n",
      "3c2cba919283: Verifying Checksum\n",
      "3c2cba919283: Download complete\n",
      "831c222b21d8: Verifying Checksum\n",
      "831c222b21d8: Download complete\n",
      "bd47987755ba: Verifying Checksum\n",
      "bd47987755ba: Download complete\n",
      "df37508d2f5c: Verifying Checksum\n",
      "df37508d2f5c: Download complete\n",
      "9019978541a7: Verifying Checksum\n",
      "9019978541a7: Download complete\n",
      "c28e7cc900d1: Verifying Checksum\n",
      "c28e7cc900d1: Download complete\n",
      "e378d88a5f59: Verifying Checksum\n",
      "e378d88a5f59: Download complete\n",
      "afebcf787e04: Verifying Checksum\n",
      "afebcf787e04: Download complete\n",
      "a0336ba74309: Verifying Checksum\n",
      "a0336ba74309: Download complete\n",
      "b32cc9704312: Verifying Checksum\n",
      "b32cc9704312: Download complete\n",
      "80dc388c898c: Verifying Checksum\n",
      "80dc388c898c: Download complete\n",
      "bd47987755ba: Pull complete\n",
      "831c222b21d8: Pull complete\n",
      "3c2cba919283: Pull complete\n",
      "e378d88a5f59: Pull complete\n",
      "df37508d2f5c: Pull complete\n",
      "c28e7cc900d1: Pull complete\n",
      "9019978541a7: Pull complete\n",
      "80dc388c898c: Pull complete\n",
      "afebcf787e04: Pull complete\n",
      "b32cc9704312: Pull complete\n",
      "a0336ba74309: Pull complete\n",
      "Digest: sha256:0700c27c6492b8b2998e7d543ca13088db8d40ef26bd5c6eec58245ff8cdec35\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:0.25.0\n",
      " ---> 05d9b228cf63\n",
      "Step 2/4 : WORKDIR /pipeline\n",
      " ---> Running in 3e05c3f189f4\n",
      "Removing intermediate container 3e05c3f189f4\n",
      " ---> b7ea7a1782b3\n",
      "Step 3/4 : COPY ./ ./\n",
      " ---> 17a41a775a98\n",
      "Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in f6f0609ab83b\n",
      "Removing intermediate container f6f0609ab83b\n",
      " ---> 4742de01467e\n",
      "Successfully built 4742de01467e\n",
      "Successfully tagged gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk\n",
      "PUSH\n",
      "Pushing gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/caip-tfx-custom]\n",
      "51f48c238106: Preparing\n",
      "39f3a508fdcf: Preparing\n",
      "5dadc0a09248: Preparing\n",
      "8fb12d3bda49: Preparing\n",
      "2471eac28ba8: Preparing\n",
      "674ba689ae71: Preparing\n",
      "4058ae03fa32: Preparing\n",
      "e3437c61d457: Preparing\n",
      "84ff92691f90: Preparing\n",
      "54b00d861a7a: Preparing\n",
      "c547358928ab: Preparing\n",
      "84ff92691f90: Preparing\n",
      "c4e66be694ce: Preparing\n",
      "47cc65c6dd57: Preparing\n",
      "674ba689ae71: Waiting\n",
      "4058ae03fa32: Waiting\n",
      "e3437c61d457: Waiting\n",
      "84ff92691f90: Waiting\n",
      "54b00d861a7a: Waiting\n",
      "c547358928ab: Waiting\n",
      "c4e66be694ce: Waiting\n",
      "47cc65c6dd57: Waiting\n",
      "8fb12d3bda49: Layer already exists\n",
      "5dadc0a09248: Layer already exists\n",
      "2471eac28ba8: Layer already exists\n",
      "674ba689ae71: Layer already exists\n",
      "4058ae03fa32: Layer already exists\n",
      "e3437c61d457: Layer already exists\n",
      "54b00d861a7a: Layer already exists\n",
      "c547358928ab: Layer already exists\n",
      "84ff92691f90: Layer already exists\n",
      "c4e66be694ce: Layer already exists\n",
      "47cc65c6dd57: Layer already exists\n",
      "39f3a508fdcf: Pushed\n",
      "51f48c238106: Pushed\n",
      "jarekk: digest: sha256:a41e2e228a5aeef088b19aecf6f853363183d8a0b78eca11ebc68dc9f5c5f54c size: 3266\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                      STATUS\n",
      "1cee92eb-8cd8-4720-a72f-aeaf10ddca16  2020-12-23T18:00:13+00:00  1M57S     gs://jk-mlops-dev_cloudbuild/source/1608746412.767462-5cd71c1c10824fdc933dc27e175b4ff1.tgz  gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/caip-tfx-custom:{USER} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create AI Platform Pipelines client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiplatform.pipelines import client\n",
    "\n",
    "aipp_client = client.Client(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the the parameters for AIPP execution and create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_connection_config = None\n",
    "pipeline_root = PIPELINE_ROOT\n",
    "\n",
    "pipeline = ann_pipeline(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=pipeline_root,\n",
    "    metadata_connection_config=metadata_connection_config,\n",
    "    project_id=PROJECT_ID,\n",
    "    project_number=PROJECT_NUMBER,\n",
    "    region=REGION,\n",
    "    vpc_name=VPC_NAME,\n",
    "    bq_dataset_name=bq_dataset_name,\n",
    "    index_display_name=index_display_name,\n",
    "    deployed_index_id=deployed_index_id,\n",
    "    min_item_frequency=min_item_frequency,\n",
    "    max_group_size=max_group_size,\n",
    "    dimensions=dimensions,\n",
    "    embeddings_gcs_location=embeddings_gcs_location\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'displayName': 'ann-pipeline-jarekk',\n",
       " 'pipelineSpec': {'deploymentConfig': {'@type': 'type.googleapis.com/ml_pipelines.PipelineDeploymentConfig',\n",
       "   'executors': {'extract_embeddings_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'args': ['--executor_class_path',\n",
       "       'extract_embeddings.extract_embeddings_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}},\n",
       "    'train_item_matching_model_executor': {'container': {'args': ['--executor_class_path',\n",
       "       'train_item_matching.train_item_matching_model_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk'}},\n",
       "    'deploy_index_executor': {'container': {'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'deploy_index.deploy_index_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk'}},\n",
       "    'create_index_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'args': ['--executor_class_path',\n",
       "       'create_index.create_index_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}},\n",
       "    'export_embeddings_executor': {'container': {'args': ['--executor_class_path',\n",
       "       'export_embeddings.export_embeddings_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}},\n",
       "    'compute_pmi_executor': {'container': {'args': ['--executor_class_path',\n",
       "       'compute_pmi.compute_pmi_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk'}}}},\n",
       "  'tasks': [{'executorLabel': 'compute_pmi_executor',\n",
       "    'inputs': {'parameters': {'max_group_size': {'runtimeValue': {'constantValue': {'intValue': '100'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}},\n",
       "      'bq_dataset': {'runtimeValue': {'constantValue': {'stringValue': 'song_embeddings'}}},\n",
       "      'min_item_frequency': {'runtimeValue': {'constantValue': {'intValue': '15'}}}}},\n",
       "    'cachingOptions': {},\n",
       "    'outputs': {'artifacts': {'item_cooc': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'taskInfo': {'name': 'compute_pmi'}},\n",
       "   {'cachingOptions': {},\n",
       "    'inputs': {'artifacts': {'item_cooc': {'outputArtifactKey': 'item_cooc',\n",
       "       'producerTask': 'compute_pmi'}},\n",
       "     'parameters': {'bq_dataset': {'runtimeValue': {'constantValue': {'stringValue': 'song_embeddings'}}},\n",
       "      'dimensions': {'runtimeValue': {'constantValue': {'intValue': '50'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}}}},\n",
       "    'executorLabel': 'train_item_matching_model_executor',\n",
       "    'taskInfo': {'name': 'train_item_matching_model'},\n",
       "    'outputs': {'artifacts': {'bq_model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'dependentTasks': ['compute_pmi']},\n",
       "   {'taskInfo': {'name': 'extract_embeddings'},\n",
       "    'outputs': {'artifacts': {'item_embeddings': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'inputs': {'parameters': {'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}},\n",
       "      'bq_dataset': {'runtimeValue': {'constantValue': {'stringValue': 'song_embeddings'}}}},\n",
       "     'artifacts': {'bq_model': {'outputArtifactKey': 'bq_model',\n",
       "       'producerTask': 'train_item_matching_model'}}},\n",
       "    'cachingOptions': {},\n",
       "    'executorLabel': 'extract_embeddings_executor',\n",
       "    'dependentTasks': ['train_item_matching_model']},\n",
       "   {'inputs': {'parameters': {'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}},\n",
       "      'gcs_location': {'runtimeValue': {'constantValue': {'stringValue': 'gs://jk-ann-staging/embeddings'}}}},\n",
       "     'artifacts': {'item_embeddings_bq': {'outputArtifactKey': 'item_embeddings',\n",
       "       'producerTask': 'extract_embeddings'}}},\n",
       "    'cachingOptions': {},\n",
       "    'outputs': {'artifacts': {'item_embeddings_gcs': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'executorLabel': 'export_embeddings_executor',\n",
       "    'dependentTasks': ['extract_embeddings'],\n",
       "    'taskInfo': {'name': 'export_embeddings'}},\n",
       "   {'cachingOptions': {},\n",
       "    'inputs': {'artifacts': {'item_embeddings': {'outputArtifactKey': 'item_embeddings_gcs',\n",
       "       'producerTask': 'export_embeddings'}},\n",
       "     'parameters': {'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}},\n",
       "      'dimensions': {'runtimeValue': {'constantValue': {'intValue': '50'}}},\n",
       "      'display_name': {'runtimeValue': {'constantValue': {'stringValue': 'Song embeddings'}}},\n",
       "      'project_number': {'runtimeValue': {'constantValue': {'stringValue': '895222332033'}}},\n",
       "      'region': {'runtimeValue': {'constantValue': {'stringValue': 'us-central1'}}}}},\n",
       "    'taskInfo': {'name': 'create_index'},\n",
       "    'executorLabel': 'create_index_executor',\n",
       "    'dependentTasks': ['export_embeddings'],\n",
       "    'outputs': {'artifacts': {'ann_index': {'artifactType': {'instanceSchema': 'title: ann_types.ANNIndex\\ntype: object\\nproperties: null\\n'}}}}},\n",
       "   {'outputs': {'artifacts': {'deployed_ann_index': {'artifactType': {'instanceSchema': 'title: ann_types.DeployedANNIndex\\ntype: object\\nproperties: null\\n'}}}},\n",
       "    'executorLabel': 'deploy_index_executor',\n",
       "    'cachingOptions': {},\n",
       "    'taskInfo': {'name': 'deploy_index'},\n",
       "    'inputs': {'parameters': {'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}},\n",
       "      'project_number': {'runtimeValue': {'constantValue': {'stringValue': '895222332033'}}},\n",
       "      'deployed_index_id': {'runtimeValue': {'constantValue': {'stringValue': 'deployed_song_embeddings_91'}}},\n",
       "      'region': {'runtimeValue': {'constantValue': {'stringValue': 'us-central1'}}},\n",
       "      'vpc_name': {'runtimeValue': {'constantValue': {'stringValue': 'default'}}}},\n",
       "     'artifacts': {'ann_index': {'outputArtifactKey': 'ann_index',\n",
       "       'producerTask': 'create_index'}}},\n",
       "    'dependentTasks': ['create_index']}],\n",
       "  'pipelineInfo': {'name': 'ann-pipeline-jarekk'},\n",
       "  'schemaVersion': 'v2alpha1',\n",
       "  'sdkVersion': '0.25.0'},\n",
       " 'labels': {'tfx_version': '0-25-0',\n",
       "  'tfx_runner': 'kubeflow_v2',\n",
       "  'tfx_py_version': '3-7'},\n",
       " 'runtimeConfig': {'gcsOutputDirectory': 'gs://jk-ann-staging/pipeline_root/ann-pipeline-jarekk'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
    "    project_id=PROJECT_ID,\n",
    "    display_name=PIPELINE_NAME,\n",
    "    default_image='gcr.io/{}/caip-tfx-custom:{}'.format(PROJECT_ID, USER))\n",
    "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
    "    config=config,\n",
    "    output_filename='pipeline.json')\n",
    "runner.compile(\n",
    "    pipeline,\n",
    "    write_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/ai/platform/pipelines/runs/ann-pipeline-jarekk-20201223180312?e=CaipPipelinesAlphaLaunch::CaipPipelinesAlphaEnabled,BackendzOverridingLaunch::BackendzOverridingEnabled,CloudAiLaunch::CloudAiEnabled&project=jk-mlops-dev\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/895222332033/locations/us-central1/pipelineJobs/ann-pipeline-jarekk-20201223180312',\n",
       " 'displayName': 'ann-pipeline-jarekk-20201223180312',\n",
       " 'createTime': '2020-12-23T18:03:13.176712Z',\n",
       " 'updateTime': '2020-12-23T18:03:13.176712Z',\n",
       " 'pipelineSpec': {'deploymentConfig': {'@type': 'type.googleapis.com/ml_pipelines.PipelineDeploymentConfig',\n",
       "   'executors': {'compute_pmi_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'compute_pmi.compute_pmi_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'create_index_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'create_index.create_index_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'deploy_index_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'deploy_index.deploy_index_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'export_embeddings_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'export_embeddings.export_embeddings_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'extract_embeddings_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'extract_embeddings.extract_embeddings_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'train_item_matching_model_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'train_item_matching.train_item_matching_model_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}}}},\n",
       "  'schemaVersion': 'v2alpha1',\n",
       "  'sdkVersion': '0.25.0',\n",
       "  'pipelineInfo': {'name': 'ann-pipeline-jarekk'},\n",
       "  'deploymentSpec': {'executors': {'compute_pmi_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'compute_pmi.compute_pmi_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'create_index_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'create_index.create_index_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'deploy_index_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'deploy_index.deploy_index_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'export_embeddings_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'export_embeddings.export_embeddings_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'extract_embeddings_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'extract_embeddings.extract_embeddings_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'train_item_matching_model_executor': {'container': {'image': 'gcr.io/jk-mlops-dev/caip-tfx-custom:jarekk',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'train_item_matching.train_item_matching_model_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}}}},\n",
       "  'tasks': [{'taskInfo': {'name': 'compute_pmi'},\n",
       "    'inputs': {'parameters': {'bq_dataset': {'runtimeValue': {'constantValue': {'stringValue': 'song_embeddings'}}},\n",
       "      'max_group_size': {'runtimeValue': {'constantValue': {'intValue': '100'}}},\n",
       "      'min_item_frequency': {'runtimeValue': {'constantValue': {'intValue': '15'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}}}},\n",
       "    'outputs': {'artifacts': {'item_cooc': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'executorLabel': 'compute_pmi_executor',\n",
       "    'cachingOptions': {}},\n",
       "   {'taskInfo': {'name': 'train_item_matching_model'},\n",
       "    'inputs': {'parameters': {'bq_dataset': {'runtimeValue': {'constantValue': {'stringValue': 'song_embeddings'}}},\n",
       "      'dimensions': {'runtimeValue': {'constantValue': {'intValue': '50'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}}},\n",
       "     'artifacts': {'item_cooc': {'producerTask': 'compute_pmi',\n",
       "       'outputArtifactKey': 'item_cooc'}}},\n",
       "    'outputs': {'artifacts': {'bq_model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'executorLabel': 'train_item_matching_model_executor',\n",
       "    'dependentTasks': ['compute_pmi'],\n",
       "    'cachingOptions': {}},\n",
       "   {'taskInfo': {'name': 'extract_embeddings'},\n",
       "    'inputs': {'parameters': {'bq_dataset': {'runtimeValue': {'constantValue': {'stringValue': 'song_embeddings'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}}},\n",
       "     'artifacts': {'bq_model': {'producerTask': 'train_item_matching_model',\n",
       "       'outputArtifactKey': 'bq_model'}}},\n",
       "    'outputs': {'artifacts': {'item_embeddings': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'executorLabel': 'extract_embeddings_executor',\n",
       "    'dependentTasks': ['train_item_matching_model'],\n",
       "    'cachingOptions': {}},\n",
       "   {'taskInfo': {'name': 'export_embeddings'},\n",
       "    'inputs': {'parameters': {'gcs_location': {'runtimeValue': {'constantValue': {'stringValue': 'gs://jk-ann-staging/embeddings'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}}},\n",
       "     'artifacts': {'item_embeddings_bq': {'producerTask': 'extract_embeddings',\n",
       "       'outputArtifactKey': 'item_embeddings'}}},\n",
       "    'outputs': {'artifacts': {'item_embeddings_gcs': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'executorLabel': 'export_embeddings_executor',\n",
       "    'dependentTasks': ['extract_embeddings'],\n",
       "    'cachingOptions': {}},\n",
       "   {'taskInfo': {'name': 'create_index'},\n",
       "    'inputs': {'parameters': {'dimensions': {'runtimeValue': {'constantValue': {'intValue': '50'}}},\n",
       "      'display_name': {'runtimeValue': {'constantValue': {'stringValue': 'Song embeddings'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}},\n",
       "      'project_number': {'runtimeValue': {'constantValue': {'stringValue': '895222332033'}}},\n",
       "      'region': {'runtimeValue': {'constantValue': {'stringValue': 'us-central1'}}}},\n",
       "     'artifacts': {'item_embeddings': {'producerTask': 'export_embeddings',\n",
       "       'outputArtifactKey': 'item_embeddings_gcs'}}},\n",
       "    'outputs': {'artifacts': {'ann_index': {'artifactType': {'instanceSchema': 'title: ann_types.ANNIndex\\ntype: object\\nproperties: null\\n'}}}},\n",
       "    'executorLabel': 'create_index_executor',\n",
       "    'dependentTasks': ['export_embeddings'],\n",
       "    'cachingOptions': {}},\n",
       "   {'taskInfo': {'name': 'deploy_index'},\n",
       "    'inputs': {'parameters': {'deployed_index_id': {'runtimeValue': {'constantValue': {'stringValue': 'deployed_song_embeddings_91'}}},\n",
       "      'project_id': {'runtimeValue': {'constantValue': {'stringValue': 'jk-mlops-dev'}}},\n",
       "      'project_number': {'runtimeValue': {'constantValue': {'stringValue': '895222332033'}}},\n",
       "      'region': {'runtimeValue': {'constantValue': {'stringValue': 'us-central1'}}},\n",
       "      'vpc_name': {'runtimeValue': {'constantValue': {'stringValue': 'default'}}}},\n",
       "     'artifacts': {'ann_index': {'producerTask': 'create_index',\n",
       "       'outputArtifactKey': 'ann_index'}}},\n",
       "    'outputs': {'artifacts': {'deployed_ann_index': {'artifactType': {'instanceSchema': 'title: ann_types.DeployedANNIndex\\ntype: object\\nproperties: null\\n'}}}},\n",
       "    'executorLabel': 'deploy_index_executor',\n",
       "    'dependentTasks': ['create_index'],\n",
       "    'cachingOptions': {}}]},\n",
       " 'state': 'PIPELINE_STATE_PENDING',\n",
       " 'labels': {'tfx_py_version': '3-7',\n",
       "  'tfx_runner': 'kubeflow_v2',\n",
       "  'tfx_version': '0-25-0'},\n",
       " 'runtimeConfig': {'gcsOutputDirectory': 'gs://jk-ann-staging/pipeline_root/ann-pipeline-jarekk'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aipp_client.create_run_from_job_spec('pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "**This is not an official Google product but sample code provided for an educational purpose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

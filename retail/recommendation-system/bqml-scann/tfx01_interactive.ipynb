{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX -  Interactive Pipeline\n",
    "\n",
    "This multi-part tutorial shows how to use Matrix Factorization algorithm in BigQuery ML to generate embeddings for items based on their cooccurrence statistics. The generated item embeddings can be then used to find similar items.\n",
    "\n",
    "The is notebook covers creating and running a TFX pipeline that performs the following steps:\n",
    "1. Compute PMI using a [Custom Python function](https://www.tensorflow.org/tfx/guide/custom_function_component) component.\n",
    "2. Train BigQuery ML matrix factorization model using a [Custom Python function](https://www.tensorflow.org/tfx/guide/custom_function_component) component.\n",
    "3. Extract the Embeddings from the BigQuery ML model to a BigQuery table using a [Custom Python function](https://www.tensorflow.org/tfx/guide/custom_function_component) component.\n",
    "4. Export the embeddings as TFRecords using the standard [BigQueryExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/extensions/google_cloud_big_query/example_gen/component/BigQueryExampleGen) component.\n",
    "5. Import the schema for the embeddings using the standard [ImporterNode](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/ImporterNode) component.\n",
    "6. Validate the embeddings against the imported schema using the standard [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) and [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) components. \n",
    "7. Create an embedding lookup SavedModel using the standard [Trainer](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Trainer) component.\n",
    "8. Push the embedding lookup model to a model registry directory using the standard [Pusher](https://www.tensorflow.org/tfx/guide/pusher) component.\n",
    "9. Build the ScaNN index using the standard [Trainer](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Trainer) component.\n",
    "10. Evaluate and validate the ScaNN index latency and recall by implementing a [TFX Custom Component](https://www.tensorflow.org/tfx/guide/custom_component).\n",
    "11. Push the ScaNN index to a model registry directory using standard [Pusher](https://www.tensorflow.org/tfx/guide/pusher) component.\n",
    "\n",
    "\n",
    "After running the pipeline steps, we check the metadata stored in the local MLMD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"TFX Version:\", tfx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure GCP environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml' # Change to your project.\n",
    "BUCKET = 'ksalama-cloudml' # Change to your bucket.\n",
    "BQ_DATASET_NAME = 'recommendations'\n",
    "ARTIFACT_STORE = f'gs://{BUCKET}/tfx_artifact_store'\n",
    "LOCAL_MLMD_SQLLITE = 'mlmd/mlmd.sqllite'\n",
    "PIPELINE_NAME = 'tfx_bqml_scann'\n",
    "EMBEDDING_LOOKUP_MODEL_NAME = 'embeddings_lookup'\n",
    "SCANN_INDEX_MODEL_NAME = 'embeddings_scann'\n",
    "\n",
    "PIPELINE_ROOT = os.path.join(ARTIFACT_STORE, f'{PIPELINE_NAME}_interactive')\n",
    "MODEL_REGISTRY_DIR = os.path.join(ARTIFACT_STORE, 'model_registry_interactive')\n",
    "\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your GCP account\n",
    "This is required if you run the notebook in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print(\"Colab user is authenticated.\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interactive Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_ARTIFACTS = True\n",
    "if CLEAN_ARTIFACTS:\n",
    "  if tf.io.gfile.exists(PIPELINE_ROOT):\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    tf.io.gfile.rmtree(PIPELINE_ROOT)\n",
    "  if tf.io.gfile.exists('mlmd'):\n",
    "    print(\"Removing local mlmd SQLite...\")\n",
    "    tf.io.gfile.rmtree('mlmd')\n",
    "\n",
    "if not tf.io.gfile.exists('mlmd'):\n",
    "  print(\"Creating mlmd directory...\")\n",
    "  tf.io.gfile.mkdir('mlmd')\n",
    "    \n",
    "print(f'Pipeline artifacts directory: {PIPELINE_ROOT}')\n",
    "print(f'Model registry directory: {MODEL_REGISTRY_DIR}')\n",
    "print(f'Local metadata SQLlit path: {LOCAL_MLMD_SQLLITE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metadata as mlmd\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = LOCAL_MLMD_SQLLITE\n",
    "connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n",
    "mlmd_store = mlmd.metadata_store.MetadataStore(connection_config)\n",
    "\n",
    "context = InteractiveContext(\n",
    "  pipeline_name=PIPELINE_NAME,\n",
    "  pipeline_root=PIPELINE_ROOT,\n",
    "  metadata_connection_config=connection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Pipeline Steps\n",
    "The pipeline BigQuery steps components are implemented in [tfx_pipeline/bq_components.py](tfx_pipeline/bq_components.py) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx_pipeline import bq_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute PMI step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_computer = bq_components.compute_pmi(\n",
    "  project_id=PROJECT_ID,\n",
    "  bq_dataset=BQ_DATASET_NAME,\n",
    "  min_item_frequency=15,\n",
    "  max_group_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(pmi_computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_computer.outputs.item_cooc.get()[0].get_string_custom_property('bq_result_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the BigQuery matrix factorization model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqml_trainer = bq_components.train_item_matching_model(\n",
    "  project_id=PROJECT_ID,\n",
    "  bq_dataset=BQ_DATASET_NAME,\n",
    "  item_cooc=pmi_computer.outputs.item_cooc,\n",
    "  dimensions=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(bqml_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqml_trainer.outputs.bq_model.get()[0].get_string_custom_property('bq_model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract trained embeddings step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_extractor = bq_components.extract_embeddings(\n",
    "  project_id=PROJECT_ID,\n",
    "  bq_dataset=BQ_DATASET_NAME,\n",
    "  bq_model=bqml_trainer.outputs.bq_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(embeddings_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_extractor.outputs.item_embeddings.get()[0].get_string_custom_property('bq_result_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export embeddings as TFRecords step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen\n",
    "\n",
    "query = f'''\n",
    "  SELECT item_Id, embedding, bias,\n",
    "  FROM {BQ_DATASET_NAME}.item_embeddings\n",
    "  LIMIT 1000\n",
    "'''\n",
    "\n",
    "output_config = example_gen_pb2.Output(\n",
    "  split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "    example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=1)])\n",
    ")\n",
    "\n",
    "embeddings_exporter = BigQueryExampleGen(\n",
    "  query=query,\n",
    "  output_config=output_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_pipeline_args = [\n",
    "  '--runner=DirectRunner',\n",
    "  f'--project={PROJECT_ID}',\n",
    "  f'--temp_location=gs://{BUCKET}/bqml_scann/beam/temp',\n",
    "]\n",
    "\n",
    "context.run(embeddings_exporter, beam_pipeline_args=beam_pipeline_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Import the Schema for the embeddings step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_importer = tfx.components.ImporterNode(\n",
    "  source_uri='tfx_pipeline/schema',\n",
    "  artifact_type=tfx.types.standard_artifacts.Schema,\n",
    "  instance_name='SchemaImporter'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(schema_importer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_importer.outputs.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a sample embedding from the exported TFRecord files using the Schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file = schema_importer.outputs.result.get()[0].uri + \"/schema.pbtxt\"\n",
    "schema = tfdv.load_schema_text(schema_file)\n",
    "feature_sepc = schema_utils.schema_as_feature_spec(schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uri = embeddings_exporter.outputs.examples.get()[0].uri + \"/train/*\"\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "  data_uri, \n",
    "  batch_size=1, \n",
    "  num_epochs=1,\n",
    "  features=feature_sepc,\n",
    "  reader=_gzip_reader_fn,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "for _ in dataset: counter +=1\n",
    "print(f'Number of records: {counter}')\n",
    "print('')\n",
    "\n",
    "for batch in dataset.take(1):\n",
    "  print(f'item: {batch[\"item_Id\"].numpy()[0][0].decode()}')\n",
    "  print(f'embedding vector: {batch[\"embedding\"].numpy()[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Validate the embeddings against the imported Schema step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_generator = tfx.components.StatisticsGen(\n",
    "  examples=embeddings_exporter.outputs.examples,\n",
    ")\n",
    "\n",
    "context.run(stats_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_validator = tfx.components.ExampleValidator(\n",
    "  statistics=stats_generator.outputs.statistics,\n",
    "  schema=schema_importer.outputs.result,\n",
    ")\n",
    "\n",
    "context.run(stats_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(stats_validator.outputs.anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create an embedding lookup SavedModel step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer import executor as trainer_executor\n",
    "\n",
    "_module_file = 'tfx_pipeline/lookup_creator.py'\n",
    "\n",
    "embedding_lookup_creator = tfx.components.Trainer(\n",
    "  custom_executor_spec=executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),\n",
    "  module_file=_module_file,\n",
    "  train_args={'splits': ['train'], 'num_steps': 0},\n",
    "  eval_args={'splits': ['train'], 'num_steps': 0},\n",
    "  schema=schema_importer.outputs.result,\n",
    "  examples=embeddings_exporter.outputs.examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(embedding_lookup_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Infra-validate the lookup model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.proto import infra_validator_pb2\n",
    "\n",
    "serving_config = infra_validator_pb2.ServingSpec(\n",
    "  tensorflow_serving=infra_validator_pb2.TensorFlowServing(\n",
    "      tags=['latest']),\n",
    "  local_docker=infra_validator_pb2.LocalDockerConfig(),\n",
    ")\n",
    "  \n",
    "validation_config = infra_validator_pb2.ValidationSpec(\n",
    "  max_loading_time_seconds=60,\n",
    "  num_tries=3,\n",
    ")\n",
    "\n",
    "infra_validator = tfx.components.InfraValidator(\n",
    "  model=embedding_lookup_creator.outputs.model,\n",
    "  serving_spec=serving_config,\n",
    "  validation_spec=validation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(infra_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.gfile.listdir(infra_validator.outputs.blessing.get()[0].uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Push the embedding lookup model to the model registry step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_lookup_pusher = tfx.components.Pusher(\n",
    "  model=embedding_lookup_creator.outputs.model,\n",
    "  infra_blessing=infra_validator.outputs.blessing,\n",
    "  push_destination=tfx.proto.pusher_pb2.PushDestination(\n",
    "    filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n",
    "      base_directory=os.path.join(MODEL_REGISTRY_DIR, EMBEDDING_LOOKUP_MODEL_NAME))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(embedding_lookup_pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_savedmodel_dir = embedding_lookup_pusher.outputs.pushed_model.get()[0].get_string_custom_property('pushed_destination')\n",
    "!saved_model_cli show --dir {lookup_savedmodel_dir} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(lookup_savedmodel_dir)\n",
    "vocab = [token.strip() for token in tf.io.gfile.GFile(\n",
    "  loaded_model.vocabulary_file.asset_path.numpy().decode(), 'r').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_items = [vocab[0], ' '.join([vocab[1], vocab[2]]), 'abc123']\n",
    "print(input_items)\n",
    "output = loaded_model(input_items)\n",
    "print(f'Embeddings retrieved: {len(output)}')\n",
    "for idx, embedding in enumerate(output):\n",
    "  print(f'{input_items[idx]}: {embedding[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Build the ScaNN index step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer import executor as trainer_executor\n",
    "\n",
    "_module_file = 'tfx_pipeline/scann_indexer.py'\n",
    "\n",
    "scann_indexer = tfx.components.Trainer(\n",
    "  custom_executor_spec=executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),\n",
    "  module_file=_module_file,\n",
    "  train_args={'splits': ['train'], 'num_steps': 0},\n",
    "  eval_args={'splits': ['train'], 'num_steps': 0},\n",
    "  schema=schema_importer.outputs.result,\n",
    "  examples=embeddings_exporter.outputs.examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(scann_indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Evaluate and validate the ScaNN Index step\n",
    "\n",
    "The IndexEvaluator custom component is implemented in the [tfx_pipeline/scann_evaluator.py](tfx_pipeline/scann_evaluator.py) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx_pipeline import scann_evaluator\n",
    "\n",
    "index_evaluator = scann_evaluator.IndexEvaluator(\n",
    "  examples=embeddings_exporter.outputs.examples,\n",
    "  model=scann_indexer.outputs.model,\n",
    "  schema=schema_importer.outputs.result,\n",
    "  min_recall=0.8,\n",
    "  max_latency=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(index_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Push the ScaNN index to the model registry step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_scann_pusher = tfx.components.Pusher(\n",
    "  model=scann_indexer.outputs.model,\n",
    "  model_blessing=index_evaluator.outputs.blessing,\n",
    "  push_destination=tfx.proto.pusher_pb2.PushDestination(\n",
    "    filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n",
    "      base_directory=os.path.join(MODEL_REGISTRY_DIR, SCANN_INDEX_MODEL_NAME))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(embedding_scann_pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index_server.matching import ScaNNMatcher\n",
    "scann_index_dir = embedding_scann_pusher.outputs.pushed_model.get()[0].get_string_custom_property('pushed_destination')\n",
    "scann_matcher = ScaNNMatcher(scann_index_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.random.rand(50)\n",
    "scann_matcher.match(vector, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Local MLMD Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmd_store.get_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Model Registry Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {MODEL_REGISTRY_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "**This is not an official Google product but sample code provided for an educational purpose**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

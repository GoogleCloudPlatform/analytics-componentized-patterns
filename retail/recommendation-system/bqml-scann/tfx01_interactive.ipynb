{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX -  Create BigQuery Stored Procedures\n",
    "\n",
    "This multi-part tutorial shows how to use Matrix Factorization algorithm in BigQuery ML to generate embeddings for items based on their cooccurrence statistics. The generated item embeddings can be then used to find similar items.\n",
    "\n",
    "The is notebook covers creating and running a TFX pipeline that performs the following steps:\n",
    "1. Compute PMI using the [Custom Python function](https://www.tensorflow.org/tfx/guide/custom_function_component) component.\n",
    "2. Train BigQuery Matrix Factorization Model using the [Custom Python function](https://www.tensorflow.org/tfx/guide/custom_function_component) component.\n",
    "3. Extract the Embeddings from the Model to a Table using the [Custom Python function](https://www.tensorflow.org/tfx/guide/custom_function_component) component.\n",
    "4. Export the embeddings as TFRecords using the [BigQueryExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/extensions/google_cloud_big_query/example_gen/component/BigQueryExampleGen) component.\n",
    "5. Create an embedding lookup SavedModel using the [Trainer](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Trainer) component.\n",
    "6. Push the embedding lookp model to a model registry directory using the [Pusher](https://www.tensorflow.org/tfx/guide/pusher) component.\n",
    "7. Build the ScaNN index using the [Trainer](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Trainer) component.\n",
    "8. Push the ScaNN index to a model registry directory using [Container-based](https://www.tensorflow.org/tfx/guide/container_component) component.\n",
    "\n",
    "\n",
    "After running the pipeline steps, we check the metadata stored in the local MLMD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"TFX Version:\", tfx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure GCP environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml'\n",
    "BUCKET = 'ksalama-cloudml'\n",
    "REGION = 'us-central1'\n",
    "BQ_DATASET_NAME = 'recommendations'\n",
    "WORKSPACE = f'gs://{BUCKET}/tfx_artifact_store/tfx_bqml_scann_interactive'\n",
    "LOCAL_MLMD_SQLLITE = 'mlmd/mlmd.sqllite'\n",
    "PIPELINE_NAME = 'bqml-scann'\n",
    "EMBEDDING_LOOKUP_MODEL_NAME = 'embeddings_lookup'\n",
    "SCANN_INDEX_MODEL_NAME = 'embeddings_scann'\n",
    "MODEL_REGISTRY_DIR = os.path.join(WORKSPACE, 'model_registry')\n",
    "\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your GCP account\n",
    "This is required if you run the notebook in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print(\"Colab user is authenticated.\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interactive Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_WORKSPACE = True\n",
    "if CLEAN_WORKSPACE:\n",
    "  if tf.io.gfile.exists(WORKSPACE):\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    tf.io.gfile.rmtree(WORKSPACE)\n",
    "  if tf.io.gfile.exists('mlmd'):\n",
    "    print(\"Removing local mlmd SQLite...\")\n",
    "    tf.io.gfile.rmtree('mlmd')\n",
    "\n",
    "if not tf.io.gfile.exists('mlmd'):\n",
    "  tf.io.gfile.mkdir('mlmd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metadata as mlmd\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = 'mlmd/mlmd.sqllite'\n",
    "connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n",
    "mlmd_store = mlmd.metadata_store.MetadataStore(connection_config)\n",
    "\n",
    "context = InteractiveContext(\n",
    "  pipeline_name=PIPELINE_NAME,\n",
    "  pipeline_root=WORKSPACE,\n",
    "  metadata_connection_config=connection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Pipeline Steps\n",
    "The pipeline BigQuery steps components are implemented in [tfx_pipeline/bq_components.py](tfx_pipeline/bq_components.py) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx_pipeline import bq_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute PMI step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_computer = bq_components.compute_pmi(\n",
    "  project_id=PROJECT_ID,\n",
    "  dataset=BQ_DATASET_NAME,\n",
    "  min_item_frequency=15,\n",
    "  max_group_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(pmi_computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_computer.outputs.item_cooc.get()[0].get_string_custom_property('bq_result_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the BigQuery matrix factorization model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqml_trainer = bq_components.train_item_matching_model(\n",
    "  project_id=PROJECT_ID,\n",
    "  dataset=BQ_DATASET_NAME,\n",
    "  item_cooc=pmi_computer.outputs.item_cooc,\n",
    "  dimensions=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(bqml_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqml_trainer.outputs.model.get()[0].get_string_custom_property('bq_model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract trained embeddings step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_extractor = bq_components.extract_embeddings(\n",
    "  project_id=PROJECT_ID,\n",
    "  dataset=BQ_DATASET_NAME,\n",
    "  model=bqml_trainer.outputs.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(embeddings_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_extractor.outputs.item_embeddings.get()[0].get_string_custom_property('bq_result_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export embeddings as TFRecords step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen\n",
    "\n",
    "fetch_embeddings_query = f'''\n",
    "  SELECT item_Id, embedding\n",
    "  FROM {BQ_DATASET_NAME}.item_embeddings\n",
    "  LIMIT 1000\n",
    "'''\n",
    "\n",
    "output_config = example_gen_pb2.Output(\n",
    "  split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "    example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=1)])\n",
    ")\n",
    "\n",
    "embeddings_exporter = BigQueryExampleGen(\n",
    "  query=fetch_embeddings_query,\n",
    "  output_config=output_config,\n",
    "  instance_name='BQExportEmbeddings'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_pipeline_args = [\n",
    "  '--runner=DirectRunner',\n",
    "  f'--project={PROJECT_ID}',\n",
    "  f'--temp_location=gs://{BUCKET}/bqml_scann/beam/temp',\n",
    "]\n",
    "\n",
    "context.run(embeddings_exporter, beam_pipeline_args=beam_pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uri = embeddings_exporter.outputs.examples.get()[0].uri + \"/train/*\"\n",
    "tfrecord_filenames = tf.data.Dataset.list_files(data_uri)\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "counter = 0\n",
    "for _ in dataset: counter +=1\n",
    "print(f'Number of records: {counter}')\n",
    "print('')\n",
    "\n",
    "# Display some records\n",
    "for tfrecord in dataset.shuffle(100).take(1):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = tf.train.Example.FromString(serialized_example)\n",
    "  item_Id = example.features.feature['item_Id'].bytes_list.value[0].decode()\n",
    "  embedding = np.array(example.features.feature['embedding'].float_list.value)\n",
    "  print(f'item: {item_Id}')\n",
    "  print(f'embedding vector: {embedding}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create an embedding lookup SavedModel step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_importer = tfx.components.ImporterNode(\n",
    "  instance_name='RawSchemaImporter',\n",
    "  source_uri='tfx_pipeline/schema',\n",
    "  artifact_type=tfx.types.standard_artifacts.Schema\n",
    ")\n",
    "\n",
    "context.run(schema_importer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_importer.outputs.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer import executor as trainer_executor\n",
    "\n",
    "_module_file = 'tfx_pipeline/lookup_exporter.py'\n",
    "\n",
    "lookup_savedmodel_exporter = tfx.components.Trainer(\n",
    "  custom_executor_spec=executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),\n",
    "  module_file=_module_file,\n",
    "  train_args={'num_steps': 0},\n",
    "  eval_args={'num_steps': 0},\n",
    "  schema=schema_importer.outputs.result,\n",
    "  examples=embeddings_exporter.outputs.examples,\n",
    "  instance_name='ExportEmbeddingLookupSavedModel'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(lookup_savedmodel_exporter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Push the embedding lookup model to the model registry step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_lookup_pusher = tfx.components.Pusher(\n",
    "  model=lookup_savedmodel_exporter.outputs.model,\n",
    "  push_destination=tfx.proto.pusher_pb2.PushDestination(\n",
    "    filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n",
    "      base_directory=os.path.join(MODEL_REGISTRY_DIR, EMBEDDING_LOOKUP_MODEL_NAME))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(embedding_lookup_pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_savedmodel_dir = embedding_lookup_pusher.outputs.pushed_model.get()[0].get_string_custom_property('pushed_destination')\n",
    "!saved_model_cli show --dir {lookup_savedmodel_dir} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(lookup_savedmodel_dir)\n",
    "vocab = [token.strip() for token in tf.io.gfile.GFile(\n",
    " loaded_model.vocabulary_file.asset_path.numpy().decode(), 'r').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_items = [vocab[0], ' '.join([vocab[1], vocab[2]]), 'abc123']\n",
    "print(input_items)\n",
    "output = loaded_model(input_items)\n",
    "print(f'Embeddings retrieved: {len(output)}')\n",
    "for idx, embedding in enumerate(output):\n",
    "  print(f'{input_items[idx]}: {embedding[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Build the ScaNN index step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer import executor as trainer_executor\n",
    "\n",
    "_module_file = 'tfx_pipeline/scann_indexer.py'\n",
    "\n",
    "scann_indexer = tfx.components.Trainer(\n",
    "  custom_executor_spec=executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),\n",
    "  module_file=_module_file,\n",
    "  train_args={'num_steps': 0},\n",
    "  eval_args={'num_steps': 0},\n",
    "  schema=schema_importer.outputs.result,\n",
    "  examples=embeddings_exporter.outputs.examples,\n",
    "  instance_name='BuildScaNNIndex'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(scann_indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Push the ScaNN index to the model registry step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_scann_pusher = tfx.components.Pusher(\n",
    "  model=scann_indexer.outputs.model,\n",
    "  push_destination=tfx.proto.pusher_pb2.PushDestination(\n",
    "    filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n",
    "      base_directory=os.path.join(MODEL_REGISTRY_DIR, SCANN_INDEX_MODEL_NAME))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(embedding_scann_pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index_server.matching import ScaNNMatcher\n",
    "scann_index_dir = embedding_scann_pusher.outputs.pushed_model.get()[0].get_string_custom_property('pushed_destination')\n",
    "scann_matcher = ScaNNMatcher(scann_index_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.random.rand(50)\n",
    "scann_matcher.match(vector, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Local MLMD Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmd_store.get_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Model Registry Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {MODEL_REGISTRY_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "**This is not an official Google product but sample code provided for an educational purpose**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

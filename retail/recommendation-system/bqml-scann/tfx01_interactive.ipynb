{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"tf2-2-3-gpu.2-3.m59","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"name":"tfx01_interactive.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"lIGxhEiFRBzM"},"source":["# Create an interactive TFX pipeline\n","\n","This notebook is the first of two notebooks that guide you through automating the [Real-time Item-to-item Recommendation with BigQuery ML Matrix Factorization and ScaNN](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/recommendation-system/bqml-scann) solution with a pipeline.\n","\n","Use this notebook to create and run a [TFX](https://www.tensorflow.org/tfx) pipeline that performs the following steps:\n","\n","1. Compute PMI on item co-occurrence data by using a [custom Python function](https://www.tensorflow.org/tfx/guide/custom_function_component) component.\n","2. Train a BigQuery ML matrix factorization model on the PMI data to learn item embeddings by using a custom Python function component.\n","3. Extract the embeddings from the model to a BigQuery table by using a custom Python function component.\n","4. Export the embeddings in [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format by using the standard [BigQueryExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/extensions/google_cloud_big_query/example_gen/component/BigQueryExampleGen) component.\n","5. Import the schema for the embeddings by using the standard [ImporterNode](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/ImporterNode) component.\n","6. Validate the embeddings against the imported schema by using the standard [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) and [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) components. \n","7. Create an embedding lookup SavedModel by using the standard [Trainer](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Trainer) component.\n","8. Push the embedding lookup model to a model registry directory by using the standard [Pusher](https://www.tensorflow.org/tfx/guide/pusher) component.\n","9. Build the ScaNN index by using the standard Trainer component.\n","10. Evaluate and validate the ScaNN index latency and recall by implementing a [TFX custom component](https://www.tensorflow.org/tfx/guide/custom_component).\n","11. Push the ScaNN index to a model registry directory by using the standard Pusher component.\n","\n","The [tfx_pipeline](tfx_pipeline) directory contains the source code for the TFX pipeline implementation. \n","\n","Before starting this notebook, you must run the [00_prep_bq_procedures](00_prep_bq_procedures.ipynb) notebook to complete the solution prerequisites.\n","\n","After completing this notebook, run the [tfx02_deploy_run](tfx02_deploy_run.ipynb) notebook to deploy the pipeline."]},{"cell_type":"markdown","metadata":{"id":"BhAa_9hBRBzP"},"source":["## Setup\r\n","\r\n","Import the required libraries, configure the environment variables, and authenticate your GCP account."]},{"cell_type":"code","metadata":{"id":"suCbts8ZRBzQ"},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmfpkqv-RBzR"},"source":["!pip install -U -q tfx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXqqJCHdRBzR"},"source":["### Import libraries"]},{"cell_type":"code","metadata":{"id":"1Bgf_d80RBzR"},"source":["import os\n","import numpy as np\n","import tfx\n","import tensorflow as tf\n","import tensorflow_data_validation as tfdv\n","from tensorflow_transform.tf_metadata import schema_utils\n","import logging\n","\n","logging.getLogger().setLevel(logging.INFO)\n","\n","print(\"Tensorflow Version:\", tf.__version__)\n","print(\"TFX Version:\", tfx.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1NPgGXqRBzS"},"source":["### Configure GCP environment settings\r\n","\r\n","Update the following variables to reflect the values for your GCP environment:\r\n","\r\n","+ `PROJECT_ID`: The ID of the Google Cloud project you are using to implement this solution.\r\n","+ `BUCKET`: The name of the Cloud Storage bucket you created to use with this solution. The `BUCKET` value should be just the bucket name, so `myBucket` rather than `gs://myBucket`."]},{"cell_type":"code","metadata":{"id":"9lAiZAzHRBzS"},"source":["PROJECT_ID = 'yourProject' # Change to your project.\n","BUCKET = 'yourBucket' # Change to the bucket you created.\n","BQ_DATASET_NAME = 'recommendations'\n","ARTIFACT_STORE = f'gs://{BUCKET}/tfx_artifact_store'\n","LOCAL_MLMD_SQLLITE = 'mlmd/mlmd.sqllite'\n","PIPELINE_NAME = 'tfx_bqml_scann'\n","EMBEDDING_LOOKUP_MODEL_NAME = 'embeddings_lookup'\n","SCANN_INDEX_MODEL_NAME = 'embeddings_scann'\n","\n","PIPELINE_ROOT = os.path.join(ARTIFACT_STORE, f'{PIPELINE_NAME}_interactive')\n","MODEL_REGISTRY_DIR = os.path.join(ARTIFACT_STORE, 'model_registry_interactive')\n","\n","!gcloud config set project $PROJECT_ID"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDcqiH6LRBzT"},"source":["### Authenticate your GCP account\n","This is required if you run the notebook in Colab. If you use an AI Platform notebook, you should already be authenticated."]},{"cell_type":"code","metadata":{"id":"_FqvtmZXRBzT"},"source":["try:\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  print(\"Colab user is authenticated.\")\n","except: pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovAIUTxmRBzU"},"source":["## Instantiate the interactive context\r\n","\r\n","Instantiate an [interactive context](https://www.tensorflow.org/tfx/api_docs/python/tfx/orchestration/experimental/interactive/interactive_context/InteractiveContext) so that you can execute the TFX pipeline components interactively in the notebook. The interactive context creates a local SQLite database in the `LOCAL_MLMD_SQLLITE` directory to use as its [ML Metadata (MLMD)](https://github.com/google/ml-metadata) store.\r\n"]},{"cell_type":"code","metadata":{"id":"fkvxex7bRBzU"},"source":["CLEAN_ARTIFACTS = True\n","if CLEAN_ARTIFACTS:\n","  if tf.io.gfile.exists(PIPELINE_ROOT):\n","    print(\"Removing previous artifacts...\")\n","    tf.io.gfile.rmtree(PIPELINE_ROOT)\n","  if tf.io.gfile.exists('mlmd'):\n","    print(\"Removing local mlmd SQLite...\")\n","    tf.io.gfile.rmtree('mlmd')\n","\n","if not tf.io.gfile.exists('mlmd'):\n","  print(\"Creating mlmd directory...\")\n","  tf.io.gfile.mkdir('mlmd')\n","    \n","print(f'Pipeline artifacts directory: {PIPELINE_ROOT}')\n","print(f'Model registry directory: {MODEL_REGISTRY_DIR}')\n","print(f'Local metadata SQLlit path: {LOCAL_MLMD_SQLLITE}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RvhwgAKERBzV"},"source":["import ml_metadata as mlmd\n","from ml_metadata.proto import metadata_store_pb2\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","\n","connection_config = metadata_store_pb2.ConnectionConfig()\n","connection_config.sqlite.filename_uri = LOCAL_MLMD_SQLLITE\n","connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n","mlmd_store = mlmd.metadata_store.MetadataStore(connection_config)\n","\n","context = InteractiveContext(\n","  pipeline_name=PIPELINE_NAME,\n","  pipeline_root=PIPELINE_ROOT,\n","  metadata_connection_config=connection_config\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxQtakfoRBzV"},"source":["## Executing the pipeline steps\n","The components that implement the pipeline steps are in the [tfx_pipeline/bq_components.py](tfx_pipeline/bq_components.py) module."]},{"cell_type":"code","metadata":{"id":"JNhuOk_ARBzW"},"source":["from tfx_pipeline import bq_components"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7mDZFagRBzW"},"source":["### Step 1: Compute PMI\r\n","\r\n","Run the `pmi_computer` step, which is an instance of the `compute_pmi` custom Python function component. This component executes the [sp_ComputePMI](sql_scripts/sp_ComputePMI.sql) stored procedure in BigQuery and returns the name of the resulting table as a custom property.\r\n"]},{"cell_type":"code","metadata":{"id":"61x6Bv-qRBzW"},"source":["pmi_computer = bq_components.compute_pmi(\n","  project_id=PROJECT_ID,\n","  bq_dataset=BQ_DATASET_NAME,\n","  min_item_frequency=15,\n","  max_group_size=100,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08R_Q3FdRBzX"},"source":["context.run(pmi_computer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGa2o4XkRBzX"},"source":["pmi_computer.outputs.item_cooc.get()[0].get_string_custom_property('bq_result_table')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHPpKQ1FRBzX"},"source":["### Step 2: Train the BigQuery ML matrix factorization model\r\n","\r\n","Run the `bqml_trainer` step, which is an instance of the `train_item_matching_model` custom Python function component. This component executes the [sp_TrainItemMatchingModel](sql_scripts/sp_TrainItemMatchingModel.sql) stored procedure in BigQuery and returns the name of the resulting model as a custom property.\r\n"]},{"cell_type":"code","metadata":{"id":"7k5M_-U_RBzX"},"source":["bqml_trainer = bq_components.train_item_matching_model(\n","  project_id=PROJECT_ID,\n","  bq_dataset=BQ_DATASET_NAME,\n","  item_cooc=pmi_computer.outputs.item_cooc,\n","  dimensions=50,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlYTLifDRBzY"},"source":["context.run(bqml_trainer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7m_hEloIRBzY"},"source":["bqml_trainer.outputs.bq_model.get()[0].get_string_custom_property('bq_model_name')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVxA1JXIRBzY"},"source":["### Step 3: Extract the trained embeddings\r\n","\r\n","Run the `embeddings_extractor` step, which is an instance of the `extract_embeddings` custom Python function component. This component executes the [sp_ExractEmbeddings](sql_scripts/sp_ExractEmbeddings.sql) stored procedure in BigQuery and returns the name of the resulting table as a custom property."]},{"cell_type":"code","metadata":{"id":"KXvjETyZRBzZ"},"source":["embeddings_extractor = bq_components.extract_embeddings(\n","  project_id=PROJECT_ID,\n","  bq_dataset=BQ_DATASET_NAME,\n","  bq_model=bqml_trainer.outputs.bq_model,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZD8fJeqeRBzZ"},"source":["context.run(embeddings_extractor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UiNteV4RBzZ"},"source":["embeddings_extractor.outputs.item_embeddings.get()[0].get_string_custom_property('bq_result_table')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDg5QqDgRBza"},"source":["### Step 4: Export the embeddings in TFRecord format\r\n","\r\n","Run the `embeddings_exporter` step, which is an instance of the `BigQueryExampleGen` standard component. This component uses a SQL query to read the embedding records from BigQuery and produces an [Examples](https://www.tensorflow.org/tfx/api_docs/python/tfx/types/standard_artifacts/Examples) artifact containing training and evaluation datasets as an output. It then exports these datasets in TFRecord format by using a Beam pipeline. This pipeline can be run using the [DirectRunner or DataflowRunner](https://beam.apache.org/documentation/runners/capability-matrix/). Note that in this interactive context, the embedding records to read is limited to 1000, and the runner of the Beam pipeline is set to DirectRunner.\r\n"]},{"cell_type":"code","metadata":{"id":"gYQFEObGRBza"},"source":["from tfx.proto import example_gen_pb2\n","from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen\n","\n","query = f'''\n","  SELECT item_Id, embedding, bias,\n","  FROM {BQ_DATASET_NAME}.item_embeddings\n","  LIMIT 1000\n","'''\n","\n","output_config = example_gen_pb2.Output(\n","  split_config=example_gen_pb2.SplitConfig(splits=[\n","    example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=1)])\n",")\n","\n","embeddings_exporter = BigQueryExampleGen(\n","  query=query,\n","  output_config=output_config\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YO69fef-RBza"},"source":["beam_pipeline_args = [\n","  '--runner=DirectRunner',\n","  f'--project={PROJECT_ID}',\n","  f'--temp_location=gs://{BUCKET}/bqml_scann/beam/temp',\n","]\n","\n","context.run(embeddings_exporter, beam_pipeline_args=beam_pipeline_args)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H04odbChRBzb"},"source":["### Step 5: Import the schema for the embeddings\r\n","\r\n","Run the `schema_importer` step, which is an instance of the `ImporterNode` standard component. This component reads the [schema.pbtxt](tfx_pipeline/schema/schema.pbtxt) file from the solution's `schema` directory, and produces a `Schema` artifact as an output. The schema is used to validate the embedding files exported from BigQuery, and to parse the embedding records in the TFRecord files when they are read in the training components.\r\n"]},{"cell_type":"code","metadata":{"id":"HXoDKq9CRBzb"},"source":["schema_importer = tfx.components.ImporterNode(\n","  source_uri='tfx_pipeline/schema',\n","  artifact_type=tfx.types.standard_artifacts.Schema,\n","  instance_name='SchemaImporter'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvFNQKJARBzb"},"source":["context.run(schema_importer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yCOqNYNRBzb"},"source":["context.show(schema_importer.outputs.result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1SqWeOaRBzc"},"source":["#### Read a sample embedding from the exported TFRecord files using the schema"]},{"cell_type":"code","metadata":{"id":"djeml3uJRBzc"},"source":["schema_file = schema_importer.outputs.result.get()[0].uri + \"/schema.pbtxt\"\n","schema = tfdv.load_schema_text(schema_file)\n","feature_sepc = schema_utils.schema_as_feature_spec(schema).feature_spec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2p2ua1pRBzc"},"source":["data_uri = embeddings_exporter.outputs.examples.get()[0].uri + \"/train/*\"\n","\n","def _gzip_reader_fn(filenames):\n","  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n","\n","dataset = tf.data.experimental.make_batched_features_dataset(\n","  data_uri, \n","  batch_size=1, \n","  num_epochs=1,\n","  features=feature_sepc,\n","  reader=_gzip_reader_fn,\n","  shuffle=True\n",")\n","\n","counter = 0\n","for _ in dataset: counter +=1\n","print(f'Number of records: {counter}')\n","print('')\n","\n","for batch in dataset.take(1):\n","  print(f'item: {batch[\"item_Id\"].numpy()[0][0].decode()}')\n","  print(f'embedding vector: {batch[\"embedding\"].numpy()[0]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iyE7IGJORBzd"},"source":["### Step 6: Validate the embeddings against the imported schema\r\n","\r\n","Runs the `stats_generator`, which is an instance of the `StatisticsGen` standard component. This component accepts the output `Examples` artifact from the `embeddings_exporter` step and computes descriptive statistics for these examples by using an Apache Beam pipeline. The component produces a `Statistics` artifact as an output."]},{"cell_type":"code","metadata":{"id":"wnvLUn5lRBze"},"source":["stats_generator = tfx.components.StatisticsGen(\n","  examples=embeddings_exporter.outputs.examples,\n",")\n","\n","context.run(stats_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CX5WVSVfGZ73"},"source":["Run the `stats_validator`, which is an instance of the `ExampleValidator` component. This component validates the output statistics against the schema. It accepts the `Statistics` artifact produced by the `stats_generator` step and the `Schema` artifact produced by the `schema_importer` step, and produces `Anomalies` artifacts as outputput if any anomalies are found."]},{"cell_type":"code","metadata":{"id":"0R7mtaPARBze"},"source":["stats_validator = tfx.components.ExampleValidator(\n","  statistics=stats_generator.outputs.statistics,\n","  schema=schema_importer.outputs.result,\n",")\n","\n","context.run(stats_validator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ExKbz0L_RBze"},"source":["context.show(stats_validator.outputs.anomalies)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3BwvlXVjRBzf"},"source":["### Step 7: Create an embedding lookup SavedModel\r\n","\r\n","Runs the `embedding_lookup_creator` step, which is an instance of the `Trainer` standard component. This component accepts the `Schema` artifact from the `schema_importer` step and the`Examples` artifact from the `embeddings_exporter` step as inputs, executes the [lookup_creator.py](tfx_pipeline/lookup_creator.py) module, and produces an embedding lookup `Model` artifact as an output.\r\n"]},{"cell_type":"code","metadata":{"id":"YX_CUruaRBzf"},"source":["from tfx.components.base import executor_spec\n","from tfx.components.trainer import executor as trainer_executor\n","\n","_module_file = 'tfx_pipeline/lookup_creator.py'\n","\n","embedding_lookup_creator = tfx.components.Trainer(\n","  custom_executor_spec=executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),\n","  module_file=_module_file,\n","  train_args={'splits': ['train'], 'num_steps': 0},\n","  eval_args={'splits': ['train'], 'num_steps': 0},\n","  schema=schema_importer.outputs.result,\n","  examples=embeddings_exporter.outputs.examples,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"InEQYZFsRBzf"},"source":["context.run(embedding_lookup_creator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOxSZZuiRBzg"},"source":["#### Validate the lookup model\r\n","\r\n","Use the [TFX InfraValidator](https://www.tensorflow.org/tfx/guide/infra_validator) to make sure the created model is mechanically fine and can be loaded successfully."]},{"cell_type":"code","metadata":{"id":"Ujlyqbt0RBzg","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1611353374505,"user_tz":480,"elapsed":593,"user":{"displayName":"Christa Carpentiere","photoUrl":"","userId":"14715348382857067059"}},"outputId":"bd5664a7-5370-43e1-fcbe-0e9d545c45f1"},"source":["from tfx.proto import infra_validator_pb2\n","\n","serving_config = infra_validator_pb2.ServingSpec(\n","  tensorflow_serving=infra_validator_pb2.TensorFlowServing(\n","      tags=['latest']),\n","  local_docker=infra_validator_pb2.LocalDockerConfig(),\n",")\n","  \n","validation_config = infra_validator_pb2.ValidationSpec(\n","  max_loading_time_seconds=60,\n","  num_tries=3,\n",")\n","\n","infra_validator = tfx.components.InfraValidator(\n","  model=embedding_lookup_creator.outputs.model,\n","  serving_spec=serving_config,\n","  validation_spec=validation_config,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yXOe5iwRBzh"},"source":["context.run(infra_validator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4xXYpFkRBzh"},"source":["tf.io.gfile.listdir(infra_validator.outputs.blessing.get()[0].uri)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dXDUaMkRBzh"},"source":["### Step 8: Push the embedding lookup model to the model registry\r\n","\r\n","Run the `embedding_lookup_pusher` step, which is an instance of the `Pusher` standard component. This component accepts the embedding lookup `Model` artifact from the `embedding_lookup_creator` step, and stores the SavedModel in the location specified by the `MODEL_REGISTRY_DIR` variable.\r\n"]},{"cell_type":"code","metadata":{"id":"KFoUKJpkRBzi"},"source":["embedding_lookup_pusher = tfx.components.Pusher(\n","  model=embedding_lookup_creator.outputs.model,\n","  infra_blessing=infra_validator.outputs.blessing,\n","  push_destination=tfx.proto.pusher_pb2.PushDestination(\n","    filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n","      base_directory=os.path.join(MODEL_REGISTRY_DIR, EMBEDDING_LOOKUP_MODEL_NAME))\n","  )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pEd8xgejRBzi"},"source":["context.run(embedding_lookup_pusher)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKCWQoDYRBzi"},"source":["lookup_savedmodel_dir = embedding_lookup_pusher.outputs.pushed_model.get()[0].get_string_custom_property('pushed_destination')\n","!saved_model_cli show --dir {lookup_savedmodel_dir} --tag_set serve --signature_def serving_default"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"736qRzRhRBzj"},"source":["loaded_model = tf.saved_model.load(lookup_savedmodel_dir)\n","vocab = [token.strip() for token in tf.io.gfile.GFile(\n","  loaded_model.vocabulary_file.asset_path.numpy().decode(), 'r').readlines()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ULGeFMeRBzj"},"source":["input_items = [vocab[0], ' '.join([vocab[1], vocab[2]]), 'abc123']\n","print(input_items)\n","output = loaded_model(input_items)\n","print(f'Embeddings retrieved: {len(output)}')\n","for idx, embedding in enumerate(output):\n","  print(f'{input_items[idx]}: {embedding[:5]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnmChMRFRBzj"},"source":["### Step 9: Build the ScaNN index\r\n","\r\n","Run the `scann_indexer` step, which is an instance of the `Trainer` standard component. This component accepts the `Schema` artifact from the `schema_importer` step and the `Examples` artifact from the `embeddings_exporter` step as inputs, executes the [scann_indexer.py](tfx_pipeline/scann_indexer.py) module, and produces the ScaNN index `Model` artifact as an output.\r\n"]},{"cell_type":"code","metadata":{"id":"-5KIy2nYRBzj"},"source":["from tfx.components.base import executor_spec\n","from tfx.components.trainer import executor as trainer_executor\n","\n","_module_file = 'tfx_pipeline/scann_indexer.py'\n","\n","scann_indexer = tfx.components.Trainer(\n","  custom_executor_spec=executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),\n","  module_file=_module_file,\n","  train_args={'splits': ['train'], 'num_steps': 0},\n","  eval_args={'splits': ['train'], 'num_steps': 0},\n","  schema=schema_importer.outputs.result,\n","  examples=embeddings_exporter.outputs.examples\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZB1YQItRBzj"},"source":["context.run(scann_indexer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45g-mNioRBzk"},"source":["### Step 10: Evaluate and validate the ScaNN index\n","\n","Runs the `index_evaluator` step, which is an instance of the `IndexEvaluator` custom TFX component. This component accepts the `Examples` artifact from the `embeddings_exporter` step, the `Schema` artifact from the `schema_importer` step, and ScaNN index `Model` artifact from the `scann_indexer` step. The IndexEvaluator component completes the following tasks:\n","\n","+ Uses the schema to parse the embedding records. \n","+ Evaluates the matching latency of the index.\n","+ Compares the recall of the produced matches with respect to the exact matches.\n","+ Validates the latency and recall against the `max_latency` and `min_recall` input parameters.\n","\n","When it is finished, it produces a `ModelBlessing` artifact as output, which indicates whether the ScaNN index passed the validation criteria or not.\n","\n","The IndexEvaluator custom component is implemented in the [tfx_pipeline/scann_evaluator.py](tfx_pipeline/scann_evaluator.py) module."]},{"cell_type":"code","metadata":{"id":"kycmXtepRBzk"},"source":["from tfx_pipeline import scann_evaluator\n","\n","index_evaluator = scann_evaluator.IndexEvaluator(\n","  examples=embeddings_exporter.outputs.examples,\n","  model=scann_indexer.outputs.model,\n","  schema=schema_importer.outputs.result,\n","  min_recall=0.8,\n","  max_latency=0.01,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OO0XRx-hRBzk"},"source":["context.run(index_evaluator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzpuQLhtRBzk"},"source":["### Step 11: Push the ScaNN index to the model registry\r\n","\r\n","Runs the `embedding_scann_pusher` step, which is an instance of the `Pusher` standard component. This component accepts the ScaNN index `Model` artifact from the `scann_indexer` step and the `ModelBlessing` artifact from the `index_evaluator` step, and stores the SavedModel in the location specified by the `MODEL_REGISTRY_DIR` variable.\r\n"]},{"cell_type":"code","metadata":{"id":"N16OZ93bRBzl"},"source":["embedding_scann_pusher = tfx.components.Pusher(\n","  model=scann_indexer.outputs.model,\n","  model_blessing=index_evaluator.outputs.blessing,\n","  push_destination=tfx.proto.pusher_pb2.PushDestination(\n","    filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n","      base_directory=os.path.join(MODEL_REGISTRY_DIR, SCANN_INDEX_MODEL_NAME))\n","  )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jQ9oZEy6RBzl"},"source":["context.run(embedding_scann_pusher)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aWf8HdIRBzl"},"source":["from index_server.matching import ScaNNMatcher\n","scann_index_dir = embedding_scann_pusher.outputs.pushed_model.get()[0].get_string_custom_property('pushed_destination')\n","scann_matcher = ScaNNMatcher(scann_index_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qeVARPQXRBzl"},"source":["vector = np.random.rand(50)\n","scann_matcher.match(vector, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ej2mP1RmRBzl"},"source":["## Check the local MLMD store "]},{"cell_type":"code","metadata":{"id":"VUeGmnuZRBzm"},"source":["mlmd_store.get_artifacts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D8YzlDHeRBzm"},"source":["## View the model registry directory"]},{"cell_type":"code","metadata":{"id":"U69QVxbDRBzm"},"source":["!gsutil ls {MODEL_REGISTRY_DIR}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dcA67n1lRBzm"},"source":["## License\n","\n","Copyright 2020 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n","\n","See the License for the specific language governing permissions and limitations under the License.\n","\n","**This is not an official Google product but sample code provided for an educational purpose**"]}]}

{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"tf2-2-3-gpu.2-3.m59","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"name":"tfx02_deploy_run.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ocb1x0zeP7Fe"},"source":["# Compile and deploy the TFX pipeline to Kubeflow Pipelines\n","\n","This notebook is the second of two notebooks that guide you through automating the [Real-time Item-to-item Recommendation with BigQuery ML Matrix Factorization and ScaNN](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/recommendation-system/bqml-scann) solution with a pipeline.\n","\n","Use this notebook to compile the TFX pipeline to a Kubeflow Pipelines (KFP) package. This process creates an Argo YAML file in a .tar.gz package, and is accomplished through the following steps:\n","\n","1. Build a custom container image that includes the solution modules.\n","2. Compile the TFX Pipeline using the TFX command-line interface (CLI).\n","3. Deploy the compiled pipeline to KFP.\n","\n","The pipeline workflow is implemented in the [pipeline.py](tfx_pipeline/pipeline.py) module. The [runner.py](tfx_pipeline/runner.py) module reads the configuration settings from the [config.py](tfx_pipeline/config.py) module, defines the runtime parameters of the pipeline,  and creates a KFP format that is executable on AI Platform pipelines. \n","\n","Before starting this notebook, you must run the [tfx01_interactive](tfx01_interactive.ipynb) notebook to create the TFX pipeline.\n"]},{"cell_type":"markdown","metadata":{"id":"qdYregr9P7Fl"},"source":["## Install required libraries"]},{"cell_type":"code","metadata":{"id":"oFAdKJSdP7Fm"},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfrHaJmyP7Fm"},"source":["!pip install -q -U kfp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C-Ibd8heP7Fn"},"source":["## Set environment variables\r\n","\r\n","Update the following variables to reflect the values for your GCP environment:\r\n","\r\n","+ `PROJECT_ID`: The ID of the Google Cloud project you are using to implement this solution.\r\n","+ `BUCKET`: The name of the Cloud Storage bucket you created to use with this solution. The `BUCKET` value should be just the bucket name, so `myBucket` rather than `gs://myBucket`.\r\n","+ `GKE_CLUSTER_NAME`: The name of the Kubernetes Engine cluster used by the AI Platform pipeline. You can find this by looking at the **Cluster** column of the `kubeflow-pipelines` pipeline instance on the AI Platform Pipelines page.\r\n","+ `GKE_CLUSTER_ZONE`: The zone of the Kubernetes Engine cluster used by the AI Platform pipeline. You can find this by looking at the **Zone** column of the `kubeflow-pipelines` pipeline instance on the AI Platform Pipelines page."]},{"cell_type":"code","metadata":{"id":"VHES7S4iP7Fn"},"source":["import os\n","\n","os.environ['PROJECT_ID'] = 'yourProject' # Set your project.\n","os.environ['BUCKET'] = 'yourBucket' # Set your bucket.\n","os.environ['GKE_CLUSTER_NAME'] = 'yourCluster' # Set your GKE cluster name.\n","os.environ['GKE_CLUSTER_ZONE'] = 'yourClusterZone' # Set your GKE cluster zone.\n","\n","os.environ['IMAGE_NAME'] = 'tfx-ml'\n","os.environ['TAG'] = 'tfx0.25.0'\n","os.environ['ML_IMAGE_URI']=f'gcr.io/{os.environ.get(\"PROJECT_ID\")}/{os.environ.get(\"IMAGE_NAME\")}:{os.environ.get(\"TAG\")}'\n","\n","os.environ['NAMESPACE'] = 'kubeflow-pipelines'\n","os.environ['ARTIFACT_STORE_URI'] = f'gs://{os.environ.get(\"BUCKET\")}/tfx_artifact_store'\n","os.environ['GCS_STAGING_PATH'] = f'{os.environ.get(\"ARTIFACT_STORE_URI\")}/staging'\n","\n","os.environ['RUNTIME_VERSION'] = '2.2'\n","os.environ['PYTHON_VERSION'] = '3.7'\n","os.environ['BEAM_RUNNER'] = 'DirectRunner'\n","os.environ['MODEL_REGISTRY_URI'] = f'{os.environ.get(\"ARTIFACT_STORE_URI\")}/model_registry'\n","\n","os.environ['PIPELINE_NAME'] = 'tfx_bqml_scann'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQua8zUuP7Fo"},"source":["from tfx_pipeline import config\n","\n","for key, value in config.__dict__.items():\n","  if key.isupper(): print(f'{key}: {value}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiOIUg-fP7Fo"},"source":["## Run the Pipeline locally by using the Beam runner"]},{"cell_type":"code","metadata":{"id":"zuheADbDP7Fp"},"source":["import kfp\n","import tfx\n","from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n","from tfx_pipeline import pipeline as pipeline_module\n","import tensorflow as tf\n","import ml_metadata as mlmd\n","from ml_metadata.proto import metadata_store_pb2\n","import logging\n","\n","logging.getLogger().setLevel(logging.INFO)\n","\n","print(\"TFX Version:\", tfx.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5UbLvJ8P7Fp"},"source":["pipeline_root = f'{config.ARTIFACT_STORE_URI}/{config.PIPELINE_NAME}_beamrunner'\n","model_regisrty_uri = f'{config.MODEL_REGISTRY_URI}_beamrunner'\n","local_mlmd_sqllite = 'mlmd/mlmd.sqllite'\n","\n","print(f'Pipeline artifacts root: {pipeline_root}')    \n","print(f'Model registry location: {model_regisrty_uri}')  \n","\n","if tf.io.gfile.exists(pipeline_root):\n","  print(\"Removing previous artifacts...\")\n","  tf.io.gfile.rmtree(pipeline_root)\n","if tf.io.gfile.exists('mlmd'):\n","  print(\"Removing local mlmd SQLite...\")\n","  tf.io.gfile.rmtree('mlmd')\n","print(\"Creating mlmd directory...\")\n","tf.io.gfile.mkdir('mlmd')\n","\n","metadata_connection_config = metadata_store_pb2.ConnectionConfig()\n","metadata_connection_config.sqlite.filename_uri = local_mlmd_sqllite\n","metadata_connection_config.sqlite.connection_mode = 3\n","print(\"ML metadata store is ready.\")\n","\n","beam_pipeline_args = [\n","  f'--runner=DirectRunner',\n","  f'--project={config.PROJECT_ID}',\n","  f'--temp_location={config.ARTIFACT_STORE_URI}/beam/tmp'\n","]\n","\n","pipeline_module.SCHEMA_DIR = 'tfx_pipeline/schema'\n","pipeline_module.LOOKUP_CREATOR_MODULE = 'tfx_pipeline/lookup_creator.py'\n","pipeline_module.SCANN_INDEXER_MODULE = 'tfx_pipeline/scann_indexer.py'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BA3IbMZZP7Fq"},"source":["runner = BeamDagRunner()\n","\n","pipeline = pipeline_module.create_pipeline(\n","  pipeline_name=config.PIPELINE_NAME,\n","  pipeline_root=pipeline_root,\n","  project_id=config.PROJECT_ID,\n","  bq_dataset_name=config.BQ_DATASET_NAME,\n","  min_item_frequency=15,\n","  max_group_size=10,\n","  dimensions=50,\n","  num_leaves=500,\n","  eval_min_recall=0.8,\n","  eval_max_latency=0.001,\n","  ai_platform_training_args=None,\n","  beam_pipeline_args=beam_pipeline_args,\n","  model_regisrty_uri=model_regisrty_uri,\n","  metadata_connection_config=metadata_connection_config,\n","  enable_cache=True\n",")\n","\n","runner.run(pipeline)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sDHQOTlzP7Fr"},"source":["## Build the container image\n","\n","The pipeline uses a custom container image, which is a derivative of the [tensorflow/tfx:0.25.0](https://hub.docker.com/r/tensorflow/tfx) image, as a runtime execution environment for the pipeline's components. The container image is defined in a [Dockerfile](tfx_pipeline/Dockerfile).\n","\n","The container image installs the required libraries and copies over the modules from the solution's [tfx_pipeline](tfx_pipeline) directory, where the custom components are implemented. The container image is also used by AI Platform Training for executing the training jobs. \n","\n","Build the container image using Cloud Build and then store it in Cloud Container Registry:\n","\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"PW_sUGUtP7Fr"},"source":["!gcloud builds submit --tag $ML_IMAGE_URI tfx_pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_bmd8YmP7Fr"},"source":["## Compile the TFX pipeline using the TFX CLI\r\n","\r\n","Use the TFX CLI to compile the TFX pipeline to the KFP format, which allows the pipeline to be deployed and executed on AI Platform Pipelines. The output is a .tar.gz package containing an Argo definition of your pipeline.\r\n"]},{"cell_type":"code","metadata":{"id":"n5QGIAclP7Fs"},"source":["!rm ${PIPELINE_NAME}.tar.gz\n","!tfx pipeline compile \\\n","    --engine=kubeflow \\\n","    --pipeline_path=tfx_pipeline/runner.py "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qo6z0fQcP7Fs"},"source":["## Deploy the compiled pipeline to KFP\r\n","\r\n","Use the KFP CLI to deploy the pipeline to a hosted instance of KFP on AI Platform Pipelines:\r\n"]},{"cell_type":"code","metadata":{"id":"baYgjczHP7Fs"},"source":["%%bash\n","\n","gcloud container clusters get-credentials ${GKE_CLUSTER_NAME} --zone ${GKE_CLUSTER_ZONE}\n","export KFP_ENDPOINT=$(kubectl describe configmap inverse-proxy-config -n ${NAMESPACE} | grep \"googleusercontent.com\")\n","\n","kfp --namespace=${NAMESPACE} --endpoint=${KFP_ENDPOINT} \\\n","    pipeline upload \\\n","    --pipeline-name=${PIPELINE_NAME} \\\n","    ${PIPELINE_NAME}.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuSbXIvdbYqL"},"source":["After deploying the pipeline, you can browse it by following these steps:\r\n","\r\n","1. Open the [AI Platform Pipelines page](https://pantheon.corp.google.com/ai-platform/pipelines/clusters).\r\n","1. For the `kubeflow-pipelines` instance, click **Open Pipelines Dashboard**.\r\n","1. Click **Pipelines** and confirm that `tfx_bqml_scann` appears on the list of pipelines."]},{"cell_type":"markdown","metadata":{"id":"YlNx68crP7Ft"},"source":["## Run the deployed pipeline\r\n","\r\n","Run the pipeline by using the KFP UI:\r\n","\r\n","1. Open the [AI Platform Pipelines page](https://pantheon.corp.google.com/ai-platform/pipelines/clusters).\r\n","1. For the `kubeflow-pipelines` instance, click **Open Pipelines Dashboard**.\r\n","1. Click **Experiments**.\r\n","1. Click **Create Run**.\r\n","1. For **Pipeline**, choose **tfx_bqml_scann** and then click **Use this pipeline**.\r\n","1. For **Pipeline Version**, choose **tfx_bqml_scann**.\r\n","1. For **Run name**, type `run of tfx_bqml_scann`.\r\n","1. For **Experiment**, choose **Default** and then click **Use this experiment**.\r\n","1. Click **Start**.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"HmudthxFlnTm"},"source":["The pipelines dashboard displays a list of pipeline runs. In the list, click the name of your run to see a graph of the run displayed. While your run is still in progress, the graph changes as each step executes. Click any step to explore the run's inputs, outputs, logs, etc."]},{"cell_type":"markdown","metadata":{"id":"b9lcrRA_P7Fu"},"source":["## License\n","\n","Copyright 2020 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n","\n","See the License for the specific language governing permissions and limitations under the License.\n","\n","**This is not an official Google product but sample code provided for an educational purpose**"]}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHLV0D7Y5jtU"
   },
   "source": [
    "<table align=\"left\">\n",
    "    <td>\n",
    "      <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=BigQuery%20ML%20-%20Retail%20Demand%20Forecasting&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fanalytics-componentized-patterns%2Fraw%2Fmaster%2Fretail%2Ftime-series%2Fbqml-demand-forecasting%2Fbqml_retail_demand_forecasting.ipynb&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fanalytics-componentized-patterns%2Ftree%2Fmaster%2Fretail%2Ftime-series%2Fbqml-demand-forecasting%2F\">\n",
    "        <img src=\"https://cloud.google.com/images/products/ai/ai-solutions-icon.svg\" alt=\"AI Platform Notebooks\">Run on AI Platform Notebooks</a>\n",
    "    </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/time-series/bqml-demand-forecasting/\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# Overview\n",
    "This notebook shows you how to train, deploy, and evaluate a time series model BigQuery ML.\n",
    "\n",
    "#### Demand forecasting in retail businesses\n",
    "In many business cases, systems may be collecting various pieces of data over time. For example, there may be historical data on items purchased by consumers over time. Often it is useful to forecast some future points based on past data, which can then help inform future business decisions based on past ones. For example, to help them stock sufficient inventory, a retail company may want to predict how many items they will sell in the coming month given their historical sales data. For these situations, time series forecasting is the right tool to use.\n",
    "\n",
    "#### Time series forecasting with BigQuery ML\n",
    "This notebook will show you how to train a time series model using BigQuery ML to fit and forecast retail sales of liquor products. With BigQuery ML, you can train, evaluate and deploy our models directly within BigQuery using SQL, which saves time from needing to manually configure ML infrastructure.\n",
    "\n",
    "#### How does time series forecasting with BigQuery ML work?\n",
    "When you train a time series model with BigQuery ML, multiple models/components are used in the model creation pipeline. [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average), is one of the core algorithms. Other components are also used, as listed  roughly in the order the steps they are run: \n",
    "\n",
    "- Automatic cleaning adjustments to the input time series, including missing values, duplicated timestamps, spike anomalies, and accounting for abrupt level changes in the time series history.\n",
    "- Holiday effects adjustments.\n",
    "- Seasonal and trend decomposition using the [Seasonal and Trend decomposition using Loess (STL)](https://otexts.com/fpp2/stl.html) algorithm.\n",
    "Seasonality extrapolation using the [double exponential smoothing (ETS)](https://en.wikipedia.org/wiki/Exponential_smoothing#Double_exponential_smoothing) algorithm.\n",
    "- Trend modeling using the [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) model and the auto.ARIMA algorithm for automatic hyper-parameter tuning. In [auto.ARIMA](https://otexts.com/fpp2/arima-r.html), dozens of candidate models are trained and evaluated in parallel, which include p,d,q and drift. The best model comes with the lowest [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion).  \n",
    "\n",
    "#### Time series modeling with multiple products\n",
    "You can train a time series model to forecast a single product, or forecast multiple products at the same time. To forecast multiple products at the same time, different pipelines are run in parallel as long as there are enough slots.\n",
    "\n",
    "More information can be found in the [BigQuery ML time series model documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series#whats_inside_a_time_series_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Sym0_-GsDkd"
   },
   "source": [
    "## Scope of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwbsbfakap9Y"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The [Iowa Liquor Sales data](https://console.cloud.google.com/marketplace/details/obfuscated-ga360-data/obfuscated-ga360-data?filter=solution-type:dataset) dataset, which is hosted publicly on BigQuery, is a dataset that \"contains the spirits purchase information of Iowa Class “E” liquor licensees by product and date of purchase from January 1, 2012 to current\" (from the [official documentation by the State of Iowa](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkWgfHmkap9Z"
   },
   "source": [
    "### Objective\n",
    "\n",
    "The goal of this notebook is to provide an end-to-end solution for forecasting multiple products. Using the liquor dataset as the example, you will create multiple time series models, using a single SQL query, where each model forecasts the retail sales of a single liquor product.\n",
    "\n",
    "By the end of this notebook, you will know how to:\n",
    "* _pre-process data_ into the correct format needed to create a demand forecasting model with ARIMA using BigQuery ML\n",
    "* _train the ARIMA model_ in BigQuery ML\n",
    "* _evaluate the model_\n",
    "* _make predictions on future demand using the model_\n",
    "* _take action on the forecasted predictions:_\n",
    "  * _create a dashboard to visualize the forecasted demand using Data Studio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKPel61oap9Z"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
    "\n",
    "* BigQuery\n",
    "* BigQuery ML\n",
    "\n",
    "Learn about [BigQuery pricing](https://cloud.google.com/bigquery/pricing), [BigQuery ML\n",
    "pricing](https://cloud.google.com/bigquery-ml/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeWsN9G3ap9a"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### PIP Install Packages and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wyy5Lbnzg5fi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "cf9a4f1f-5eae-4748-c3ab-8f989f9896a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-bigquery in /opt/conda/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already up-to-date: google-cloud-bigquery-storage in /opt/conda/lib/python3.7/site-packages (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: six<2.0.0dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.22.4)\n",
      "Requirement already satisfied, skipping upgrade: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.10.2)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.4.3)\n",
      "Requirement already satisfied, skipping upgrade: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (0.3.13)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.12.0->google-cloud-bigquery) (49.6.0.post20200814)\n",
      "Requirement already satisfied, skipping upgrade: google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (1.22.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (1.31.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (3.7.4.2)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery) (1.14.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-bigquery google-cloud-bigquery-storage --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9luQpONrzPb6",
    "outputId": "8201a985-0654-471d-9199-06c2585a19cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your GCP project\n",
    "\n",
    "_The following steps are required, regardless of your notebook environment._\n",
    "\n",
    "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "1. [Enable the AI Platform APIs and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n",
    "\n",
    "1. Enter your project ID and region in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgWHxHS-sDkl"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"my-project-id\"\n",
    "REGION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKeTQnDdap9n"
   },
   "source": [
    "### Function to help us plot line graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0rsmNE1ap9n"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_historical_and_forecast(input_timeseries, \n",
    "                                 timestamp_col_name, \n",
    "                                 data_col_name, \n",
    "                                 forecast_output=None, \n",
    "                                 actual=None, \n",
    "                                 title=None,\n",
    "                                 plotstartdate=None):\n",
    "\n",
    "    if plotstartdate:\n",
    "        input_timeseries = input_timeseries[input_timeseries[timestamp_col_name] >= pd.to_datetime(plotstartdate)]\n",
    "    input_timeseries = input_timeseries.sort_values(timestamp_col_name)    \n",
    "    \n",
    "    # Plot the input historical data\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(input_timeseries[timestamp_col_name], input_timeseries[data_col_name], label = 'Historical')\n",
    "    plt.xlabel(timestamp_col_name)\n",
    "    plt.ylabel(data_col_name)\n",
    "\n",
    "    if forecast_output is not None:\n",
    "        forecast_output = forecast_output.sort_values('forecast_timestamp')\n",
    "        forecast_output['forecast_timestamp'] = pd.to_datetime(forecast_output['forecast_timestamp'])\n",
    "        x_data = forecast_output['forecast_timestamp']\n",
    "        y_data = forecast_output['forecast_value']\n",
    "        confidence_level = forecast_output['confidence_level'].iloc[0] * 100\n",
    "        low_CI = forecast_output['confidence_interval_lower_bound']\n",
    "        upper_CI = forecast_output['confidence_interval_upper_bound']\n",
    "        # Plot the forecast data\n",
    "        plt.plot(x_data, y_data, alpha = 1, label = 'Forecast', linestyle='--')\n",
    "        # Shade the confidence interval\n",
    "        plt.fill_between(x_data, low_CI, upper_CI, color = '#539caf', alpha = 0.4, \n",
    "                         label = f'{confidence_level} confidence interval')\n",
    "\n",
    "    # Plot actual data\n",
    "    if actual is not None:\n",
    "        actual = actual.sort_values(timestamp_col_name)\n",
    "        plt.plot(actual[timestamp_col_name], actual[data_col_name], label = 'Actual', linestyle='--')   \n",
    "\n",
    "    # Display title, legend\n",
    "    plt.title(f'{title}', fontsize= 16)\n",
    "    plt.legend(loc = 'upper center', prop={'size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znIaF2QGsDkr"
   },
   "source": [
    "### Creating a BigQuery dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1waFXGXsDkr"
   },
   "source": [
    "In this notebook, you will need to create a dataset in your project called `bqmlforecast`. To create it, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIBm-lrVsDks",
    "outputId": "a600c3cd-4bc1-47f4-9a29-69c0c2e9e0b1"
   },
   "outputs": [],
   "source": [
    "!bq mk --location=$REGION --dataset $PROJECT_ID:bqmlforecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB7wXYDpsDkv"
   },
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsxSG7yfsDkv"
   },
   "source": [
    "In this notebook, you will be training time series models based on a raw dataset containing transactional data. Each row represents a transaction on a single product denoted in `item_name` and `item_name` (e.g., item_name _\"36308\"_ for _\"Hawkeye Vodka\"_), and additional details such as the total number of bottles sold, `bottles_sold`, and sale in dollars, `sale_dollars`.\n",
    "\n",
    "To forecast product demand, you will focus on `bottles_sold` in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3G-0P1vsDkv"
   },
   "source": [
    "_Note_: Jupyter runs cells starting with %%bigquery as SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "cQlwqRKvap9u",
    "outputId": "9a97df3a-62c7-4e8e-8fef-01d4f8ff0d2f"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT_ID\n",
    "\n",
    "SELECT \n",
    "    invoice_and_item_number,\n",
    "    date,\n",
    "    store_number,\n",
    "    item_description,\n",
    "    bottles_sold,\n",
    "    sale_dollars\n",
    "FROM\n",
    "  `bigquery-public-data.iowa_liquor_sales.sales` \n",
    "LIMIT \n",
    "  5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiKvVQUAap9w"
   },
   "source": [
    "## [Optional] Match your dataset to template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mVjF5WYap9x"
   },
   "source": [
    "If you use the example data, you can skip this step.\n",
    "\n",
    "This tutorial assumes that you have a dump of your sales data already available in BigQuery located at [YOUR_PROJECT].[YOUR_DATASET].[YOUR_SOURCE_TABLE]\n",
    "\n",
    "You are free to adapt the SQL query in the next cell to a SQL statement that transforms your data according to the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kueioxtrap9x"
   },
   "outputs": [],
   "source": [
    "# %%bigquery --project $PROJECT_ID\n",
    "\n",
    "# CREATE OR REPLACE VIEW bqmlforecast.training_data AS (\n",
    "# SELECT\n",
    "#   date,\n",
    "#   item_name,\n",
    "#   total_amount_sold\n",
    "# FROM\n",
    "#   `[YOUR_PROJECT].[YOUR_DATASET].[YOUR_SOURCE_TABLE]`\n",
    "# );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFq6aLFBap9z"
   },
   "source": [
    "#### Set parameters for ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWbaGiY-ap90"
   },
   "source": [
    "You can adjust these parameters to specify the start/end dates of your training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2cyW1P8ap90",
    "outputId": "7253e1fd-6164-438b-a30b-53c49e61d6b2"
   },
   "outputs": [],
   "source": [
    "ARIMA_PARAMS = {\n",
    "    'TRAININGDATA_STARTDATE': '2016-01-01',\n",
    "    'TRAININGDATA_ENDDATE': '2017-06-01',\n",
    "}\n",
    "ARIMA_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXCthvMvap93"
   },
   "source": [
    "You can train ARIMA models on multiple products using the same query. In this notebook, you will train a single ARIMA model to make forecasts on 5 products (`item_name`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmz_Wh7yap93"
   },
   "source": [
    "#### Prepare the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1B5NDEfVL2C"
   },
   "source": [
    "As you may observe while preparing the training data below, there are missing dates below (i.e. days with no transactions for the product). \n",
    "\n",
    "Without needing to do extra pre-processing yourself, BigQuery ML will automatically handle:\n",
    "- _missing values_: these are imputed using local linear interpolation\n",
    "- _duplicated timestamps_: values averaged across duplicated timestamps\n",
    "- _spike and dip anomalies_: detected using local z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "7-bBYO-Hap93",
    "outputId": "59fd1987-a01b-4e3a-9719-456f790bedf3"
   },
   "outputs": [],
   "source": [
    "%%bigquery --params $ARIMA_PARAMS  --project $PROJECT_ID \n",
    "\n",
    "CREATE OR REPLACE TABLE bqmlforecast.training_data AS (\n",
    "    WITH topsellingitems AS(\n",
    "         SELECT \n",
    "            item_description,\n",
    "            count(item_description) cnt_transactions\n",
    "        FROM\n",
    "            `bigquery-public-data.iowa_liquor_sales.sales` \n",
    "        GROUP BY \n",
    "            item_description\n",
    "        ORDER BY cnt_transactions DESC\n",
    "        LIMIT 5 #Top N\n",
    "    )\n",
    "    SELECT \n",
    "        date,\n",
    "        item_description AS item_name,\n",
    "        SUM(bottles_sold) AS total_amount_sold\n",
    "    FROM\n",
    "        `bigquery-public-data.iowa_liquor_sales.sales` \n",
    "    GROUP BY\n",
    "        date, item_name\n",
    "    HAVING \n",
    "        date BETWEEN @TRAININGDATA_STARTDATE AND @TRAININGDATA_ENDDATE\n",
    "        AND item_description IN (SELECT item_description FROM topsellingitems)\n",
    "    );\n",
    "\n",
    "SELECT \n",
    "    date,\n",
    "    item_name,\n",
    "    total_amount_sold\n",
    "FROM \n",
    "    bqmlforecast.training_data \n",
    "ORDER BY item_name, date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liuS8qCnap96"
   },
   "source": [
    "#### Plot historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQxRDGLAap97"
   },
   "source": [
    "To visualize the data in Python, first save the data to a Pandas dataframe, `dfhistorical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwVPZP3fap97"
   },
   "outputs": [],
   "source": [
    "%%bigquery dfhistorical --project $PROJECT_ID \n",
    "SELECT \n",
    "    * \n",
    "FROM \n",
    "    bqmlforecast.training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiRjcr-Uap99"
   },
   "source": [
    "Plot the historical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hCvY292Xap9-",
    "outputId": "3b464686-cc5a-4ad7-f860-860ffd919f17"
   },
   "outputs": [],
   "source": [
    "itemslist = list(dfhistorical.item_name.unique())\n",
    "\n",
    "for item in itemslist:\n",
    "    \n",
    "    datah = dfhistorical[dfhistorical.item_name==item]\n",
    "    plot_historical_and_forecast(input_timeseries = datah, \n",
    "                                 timestamp_col_name = \"date\", \n",
    "                                 data_col_name = \"total_amount_sold\", \n",
    "                                 forecast_output = None, \n",
    "                                 actual = None,\n",
    "                                 title = item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft6ioX_kap-A"
   },
   "source": [
    "#### Train the time series model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqGEpOzvW2pl"
   },
   "source": [
    "Since you are training the model on multiple products in a single model creation statement, you will need to specify the parameter `TIME_SERIES_ID_COL` as `item_name`. Note that if you were only forecasting a single item, then you would not need to specify `TIME_SERIES_ID_COL`. For more information, see the [BigQuery ML time series model creation documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series#create_model_syntax).\n",
    "\n",
    "Time series modeling in BigQuery ML can also account for holiday effects. By default, holiday effects modeling is disabled. But since this data is from the United States, and the data includes a minimum one year of daily data, you can also specify an optional `HOLIDAY_REGION`. With holiday effects enabled, spike and dip anomalies that appear during holidays will no longer be treated as anomalies. A full list of the holiday regions can be found in the [HOLIDAY_REGION documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series#holiday_region).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 31
    },
    "id": "CibqJ8yVL6kS",
    "outputId": "6d8e6996-557a-4fd7-d8b1-22e8f3c9b075"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT_ID \n",
    "\n",
    "CREATE OR REPLACE MODEL bqmlforecast.arima_model\n",
    "\n",
    "OPTIONS(\n",
    "  MODEL_TYPE='ARIMA',\n",
    "  TIME_SERIES_TIMESTAMP_COL='date', \n",
    "  TIME_SERIES_DATA_COL='total_amount_sold',\n",
    "  TIME_SERIES_ID_COL='item_name',\n",
    "  HOLIDAY_REGION='US'\n",
    ") AS\n",
    "\n",
    "SELECT \n",
    "    date,\n",
    "    item_name,\n",
    "    total_amount_sold\n",
    "FROM\n",
    "  bqmlforecast.training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUkZYuZH-rn2"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vEV-65m-0QJ"
   },
   "source": [
    "You can use the `ML.EVALUATE` function to see the evaluation metrics of all the created models. \n",
    "\n",
    "The following four columns (`non_seasonal_`{`p`,`d`,`q`} and `has_drift`) define an ARIMA model. The three metrics after that (`log_likelihood`, `AIC`, and `variance`) are relevant to the ARIMA model fitting process. The fitting process determines the best ARIMA model by using the auto.ARIMA algorithm, one for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "NAlyHOOjBxRV",
    "outputId": "c40ebf67-a621-46b0-c9db-e14e907b1cd5"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT_ID \n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  ML.EVALUATE(MODEL bqmlforecast.arima_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXY99nSmCnsB"
   },
   "source": [
    "As you can see, there were five models trained, one for each of the products in _item_name_. Each model has its own p,d,q hyperparameters for ARIMA, and the detected seasonality for these five models was _WEEKLY_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mDSGlrDap-C"
   },
   "source": [
    "### Make predictions using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZrj49Nwap-D"
   },
   "source": [
    "Make predictions using `ML.FORECAST` ([syntax documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-forecast)), which forecasts the next n values, as set in `horizon`. You can also change the `confidence_level`, the percentage that the forecasted values fall within the prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTwgjaITYOx8"
   },
   "outputs": [],
   "source": [
    "%%bigquery dfforecast --project $PROJECT_ID \n",
    "\n",
    "DECLARE HORIZON STRING DEFAULT \"30\"; #number of values to forecast\n",
    "DECLARE CONFIDENCE_LEVEL STRING DEFAULT \"0.90\";\n",
    "\n",
    "EXECUTE IMMEDIATE format(\"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM \n",
    "      ML.FORECAST(MODEL bqmlforecast.arima_model, \n",
    "                  STRUCT(%s AS horizon, \n",
    "                         %s AS confidence_level)\n",
    "                 )\n",
    "    \"\"\",HORIZON,CONFIDENCE_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "YaHX0NWXap-G",
    "outputId": "5bd2bc73-4362-458d-c191-17b07c2dc912"
   },
   "outputs": [],
   "source": [
    "dfforecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N11NZ1d3ap-I"
   },
   "source": [
    "Since `horizon` is set to 30, the result is 30 x (number of items), with one row per forecasted value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5nKk-XQap-I",
    "outputId": "2246f362-c0a7-4bb8-b432-88e066276f7c"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {dfforecast.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the ARIMA model coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QLmtPK19VLE"
   },
   "source": [
    "You can view the coefficients of each of the ARIMA models using `ML.ARIMA_COEFFICIENTS` ([documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-arima-coefficients)).\n",
    "\n",
    "For each of the models, ar_coefficients shows the model coefficients of the autoregressive (AR) part of the ARIMA model. Similarly, ma_coefficients shows the model coefficients of moving-average (MA) part. They are both arrays, whose lengths are equal to non_seasonal_p and non_seasonal_q, respectively. The intercept_or_drift is the constant term in the ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "fzxhgxY6-EHs",
    "outputId": "389a2380-c920-447a-f1bd-447b0592a36e"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT_ID \n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM \n",
    "  ML.ARIMA_COEFFICIENTS(MODEL bqmlforecast.arima_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPdfnofgap-K"
   },
   "source": [
    "#### Plot the forecasted predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK9AeZnFap-L"
   },
   "source": [
    "Plot the forecasted predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PBKzdLi-ap-L",
    "outputId": "08b2165f-7f12-4144-87aa-c4c0363a020f"
   },
   "outputs": [],
   "source": [
    "itemslist = list(dfhistorical.item_name.unique())\n",
    "\n",
    "for item in itemslist:\n",
    "    datah = dfhistorical[dfhistorical.item_name==item]\n",
    "    dataf = dfforecast[dfforecast.item_name==item]\n",
    "    \n",
    "    plot_historical_and_forecast(input_timeseries = datah, \n",
    "                                 timestamp_col_name = \"date\", \n",
    "                                 data_col_name = \"total_amount_sold\", \n",
    "                                 forecast_output = dataf, \n",
    "                                 actual = None,\n",
    "                                 title = item,\n",
    "                                 plotstartdate = \"2017-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UEsngQXap-N"
   },
   "source": [
    "#### Plot the forecasted predictions against the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNPEn3ROap-O"
   },
   "source": [
    "To visualize the data in Python, first save the data to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-K52as0ap-O"
   },
   "outputs": [],
   "source": [
    "%%bigquery dfactual --params $ARIMA_PARAMS --project $PROJECT_ID \n",
    "\n",
    "DECLARE HORIZON STRING DEFAULT \"30\"; #number of values to forecast\n",
    "\n",
    "SELECT \n",
    "    date,\n",
    "    item_description AS item_name,\n",
    "    SUM(bottles_sold) AS total_amount_sold\n",
    "FROM\n",
    "    `bigquery-public-data.iowa_liquor_sales.sales` \n",
    "GROUP BY\n",
    "    date, item_name\n",
    "HAVING \n",
    "    date BETWEEN DATE_ADD(@TRAININGDATA_ENDDATE, \n",
    "                              INTERVAL 1 DAY) \n",
    "            AND DATE_ADD(@TRAININGDATA_ENDDATE, \n",
    "                             INTERVAL 1+CAST(HORIZON AS INT64) DAY) \n",
    "ORDER BY\n",
    "    date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "8dZ8pRewap-Q",
    "outputId": "79967295-00f2-431d-e482-feaf517441c3"
   },
   "outputs": [],
   "source": [
    "dfactual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8gcqtwiap-S"
   },
   "source": [
    "Plot the forecasted predictions against the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5C9tXEvwap-S",
    "outputId": "bec48383-c501-4a12-ed34-fd9b12093bd8"
   },
   "outputs": [],
   "source": [
    "itemslist = list(dfhistorical.item_name.unique())\n",
    "\n",
    "for item in itemslist:\n",
    "    datah = dfhistorical[dfhistorical.item_name==item].sort_values('date')\n",
    "    dataf = dfforecast[dfforecast.item_name==item].sort_values(['forecast_timestamp'])\n",
    "    dataa = dfactual[dfactual.item_name==item].sort_values('date')\n",
    "    plot_historical_and_forecast(input_timeseries = datah, \n",
    "                             timestamp_col_name = \"date\", \n",
    "                             data_col_name = \"total_amount_sold\", \n",
    "                             forecast_output = dataf, \n",
    "                             actual = dataa,\n",
    "                             title = item,\n",
    "                             plotstartdate = \"2017-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_rLvjgIap-U"
   },
   "source": [
    "## What to do with the forecasted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-KFF8wap-U"
   },
   "source": [
    "### Create a dashboard with Data Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuRcoFQdXbrb"
   },
   "source": [
    "Follow the steps below to create an interactive, shareable dashboard of the forecasted data using Data Studio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuiXk7b5ap-V"
   },
   "source": [
    "1. Create a view that concatenates the historical time series and the forecasted time series as shown in the following example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "2C8Wbu--kvf0",
    "outputId": "92ed97b5-b5ce-4f61-a1e6-8eed6b78cf34"
   },
   "outputs": [],
   "source": [
    "%%bigquery --params $ARIMA_PARAMS  --project $PROJECT_ID \n",
    "\n",
    "CREATE OR REPLACE VIEW bqmlforecast.outputdata_datastudio AS (\n",
    "  SELECT\n",
    "    date AS timestamp,\n",
    "    item_name,\n",
    "    total_amount_sold AS history_value,\n",
    "    NULL AS forecast_value,\n",
    "    NULL AS prediction_interval_lower_bound,\n",
    "    NULL AS prediction_interval_upper_bound\n",
    "  FROM\n",
    "    bqmlforecast.training_data\n",
    "  UNION ALL\n",
    "  SELECT\n",
    "    EXTRACT(DATE\n",
    "    FROM\n",
    "      forecast_timestamp) AS timestamp,\n",
    "    item_name,\n",
    "    NULL AS history_value,\n",
    "    forecast_value,\n",
    "    prediction_interval_lower_bound,\n",
    "    prediction_interval_upper_bound\n",
    "  FROM\n",
    "    ML.FORECAST(MODEL bqmlforecast.arima_model,\n",
    "      STRUCT(30 AS horizon, 0.9 AS confidence_level)) \n",
    "  ORDER BY timestamp\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL before the `UNION ALL` clause forms the history time series. The SQL after the `UNION ALL` clause uses `ML.FORECAST` to generate the forecasted time series as well as the prediction interval. This example uses different fields for `history_value` and `forecasted_value` to plot them in different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the [BigQuery UI](https://console.cloud.google.com/bigquery), navigate to your view (`bqmlforecast.outputdata_datastudio`) and click **Export** to **Explore with Data Studio**. A new tab opens in the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bq_export_datastudio.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the **Chart** panel, find the **Time series chart** icon and click it, as shown in the following screenshot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/datastudio_charts.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Under the **Chart** panel, in the **Data** panel, find the **Metric** section. Add the following metrics: `history_value`, `forecast_value`, `prediction_interval_lower_bound`, and `prediction_interval_upper_bound`. Then, remove the default metric **Record Count** as shown in the following screenshot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/datastudio_chartsettings.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In the **Style** panel, scroll down to the **Missing Data** option and use **Linear Interpolation** (or **Line Breaks**) instead of **Line to Zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/datastudio_missingdata.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. In the **Filter** panel, add `item_name`, and select a single liquor product (e.g., **Five O'clock Vodka**) to inspect the time series data for just that product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/datastudio_filter_item.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. After you complete these steps, the following plot appears in the left panel. The input history time series is in blue, while the forecasted series is in green. The prediction interval is the region between the lower bound series and the upper bound series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/datastudio_fiveoclockvodka.png\" align=\"left\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "arima_retail_iowaliquorsales_v0.5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
